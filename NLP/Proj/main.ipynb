{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86e0613",
   "metadata": {},
   "source": [
    "# 1. Perprocess\n",
    "## 1.1 parse XML\n",
    "use BeautifulSoup to parse XML file, output with pandas dataframe \"sentence, E#A, sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1371924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML(path):\n",
    "    with open(path) as xmldata:\n",
    "        soup = BeautifulSoup(xmldata, \"xml\")\n",
    "    # Create empty lists to store the extracted data\n",
    "    sentence_ids = []\n",
    "    texts = []\n",
    "    categories = []\n",
    "    polarities = []\n",
    "    # Loop through the 'sentence' elements and extract the necessary information\n",
    "    for sentence in soup.find_all('sentence'):\n",
    "        opinions = sentence.find('Opinions')\n",
    "        if opinions is not None:\n",
    "            s_categories = []\n",
    "            s_polarities = []\n",
    "            for opinion in opinions.find_all('Opinion'):\n",
    "                s_categories.append(opinion['category'])\n",
    "                s_polarities.append(opinion['polarity'])\n",
    "            categories.append(s_categories)\n",
    "            polarities.append(s_polarities)\n",
    "        else:\n",
    "            continue\n",
    "        sentence_ids.append(sentence['id'])\n",
    "        texts.append(sentence.find('text').text)\n",
    "\n",
    "    # Create a pandas dataframe from the extracted data\n",
    "    df = pd.DataFrame({'Sentence ID': sentence_ids,\n",
    "                       'Text': texts,\n",
    "                       'Categories': categories,\n",
    "                       'Polarities': polarities})\n",
    "    print(f'{path.split(\"/\")[-1]} has been parsed. The number of sentences with opinions is '\n",
    "          f\"{len(soup.find_all('Opinions'))}({len(soup.find_all('sentence'))}).\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "158ec316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data analysis\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Split categories and create new rows\n",
    "# df_copy = df.copy()\n",
    "# split_categories = df_copy['Category'].str.split(',')\n",
    "# df_copy = df_copy.assign(Category=split_categories).explode('Category')\n",
    "\n",
    "# # Get the unique categories and their frequencies\n",
    "# category_counts = df_copy['Category'].value_counts()\n",
    "# #top_categories = category_counts.head(50)\n",
    "\n",
    "# # Plot the frequencies in descending order\n",
    "# category_counts.plot(kind='barh')\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title('Category Frequencies')\n",
    "# plt.xlabel('Frequency')\n",
    "# plt.ylabel('Category')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8301b3c",
   "metadata": {},
   "source": [
    "## 1.2 tokenisation, (stopwords,punc removal), lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "381ae0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenisation(data):\n",
    "    tokenised_data = [word_tokenize(sentence) for sentence in data]\n",
    "    return tokenised_data\n",
    "\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stopwords = stopwords.words('english')\n",
    "    filtered_words = [[word for word in sentence if word.lower() not in stopwords] for sentence in data]\n",
    "    return filter_words\n",
    "#print(stopwords.words('english'))\n",
    "def remove_punctuation(data):\n",
    "    text = [[word for word in sentence if re.sub(r'[^\\w\\s]+', '', word).isalnum()] for sentence in data]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b3ce357",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lemmatization\n",
    "from nltk import stem\n",
    "def lemmatization(data):\n",
    "    wnl = stem.WordNetLemmatizer()\n",
    "    lematized = [[wnl.lemmatize(word) for word in sentence] for sentence in tokenised_data]\n",
    "    return lematized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dc9ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = df['Text'].values.tolist()\n",
    "#data = tokenisation(data)\n",
    "#data = remove_stopwords(data)\n",
    "#data = remove_punctuation(data)\n",
    "#data = lemmatization(data)\n",
    "#data = [[word.lower() for word in sentence] for sentence in data]\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a6da9",
   "metadata": {},
   "source": [
    "# 2. Model - Max Entropy Classifier with threshold t\n",
    "## 2.1 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962b306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import pos_tag\n",
    "# pos_data = [pos_tag(sentence,tagset='universal') for sentence in data]\n",
    "# print(pos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee38f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da64739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bow_around_verb(sentences):\n",
    "    sentence_list = []\n",
    "    verb_bow_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        verb_bow = []\n",
    "        pos_tags = nltk.pos_tag(sentence)\n",
    "\n",
    "        verbs = [word for word, pos in pos_tags if pos.startswith('VB')]\n",
    "\n",
    "        if len(verbs) > 0:\n",
    "            verb = verbs[0]\n",
    "            verb_index = sentence.index(verb)\n",
    "\n",
    "            for i in range(max(0, verb_index - 5), verb_index):\n",
    "                if pos_tags[i][1].startswith('JJ') or pos_tags[i][1].startswith('RB') or pos_tags[i][1].startswith('NN'):\n",
    "                    verb_bow.append(sentence[i])\n",
    "\n",
    "            for i in range(verb_index + 1, min(verb_index + 6, len(sentence))):\n",
    "                if pos_tags[i][1].startswith('JJ') or pos_tags[i][1].startswith('RB') or pos_tags[i][1].startswith('NN'):\n",
    "                    verb_bow.append(sentence[i])\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "        verb_bow_list.append(verb_bow)\n",
    "\n",
    "    df = pd.DataFrame({'Sentence': sentence_list, 'BoW around Verb': verb_bow_list})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a99fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bow_at_end_of_sentence(sentences):\n",
    "    sentence_list = []\n",
    "    end_of_sentence_bow_list = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        end_of_sentence_bow = []\n",
    "        pos_tags = nltk.pos_tag(sentence)\n",
    "\n",
    "        for i in range(len(pos_tags)-1, max(len(pos_tags)-6, -1), -1):\n",
    "            if pos_tags[i][1].startswith('JJ') or pos_tags[i][1].startswith('RB'):\n",
    "                end_of_sentence_bow.append(sentence[i])\n",
    "\n",
    "        sentence_list.append(sentence)\n",
    "        end_of_sentence_bow_list.append(end_of_sentence_bow)\n",
    "\n",
    "    df = pd.DataFrame({'Sentence': sentence_list, 'BoW at End of Sentence': end_of_sentence_bow_list})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98aafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    # Tokenize the text using NLTK's word_tokenize function\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if re.sub(r'[^\\w\\s]+', '', word).isalnum()]\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]   \n",
    "    return tokens\n",
    "\n",
    "def feature_unigram(df, df_t):    \n",
    "    # Create a CountVectorizer instance to compute unigram counts\n",
    "    count_vectorizer = CountVectorizer(tokenizer=custom_tokenizer,max_features=1000)\n",
    "    # Fit and transform the sentences to obtain the unigram count features\n",
    "    count_matrix = count_vectorizer.fit_transform(df['text'])\n",
    "    count_matrix_t = count_vectorizer.transform(df_t['text'])\n",
    "    # Convert the count matrix to a dataframe with appropriate column names\n",
    "    count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "    count_df_t = pd.DataFrame(count_matrix_t.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "    return count_df, count_df_t\n",
    "\n",
    "def feature_Tfidf(df, df_t):\n",
    "    # Create a TfidfVectorizer instance to compute TF-IDF features\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "    # Fit and transform the sentences to obtain the TF-IDF features\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "    tfidf_matrix_t = tfidf_vectorizer.transform(df_t['text'])\n",
    "    # Convert the TF-IDF matrix to a dataframe with appropriate column names\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    tfidf_df_t = pd.DataFrame(tfidf_matrix_t.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    return tfidf_df, tfidf_df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb949a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catagory_extraction_LogisticRegressionClf():\n",
    "    #data preprocessing\n",
    "    Laptops_Train_p1 = parseXML(\"./data/Laptops_Train_p1.xml\")\n",
    "    Laptops_Test_p1_gold = parseXML(\"./data/Laptops_Test_p1_gold.xml\")\n",
    "\n",
    "    train = pd.DataFrame({'text': Laptops_Train_p1['Text'],\n",
    "                          'label': Laptops_Train_p1['Categories']})\n",
    "    test = pd.DataFrame({'text': Laptops_Test_p1_gold['Text'],\n",
    "                         'label': Laptops_Test_p1_gold['Categories']})\n",
    "\n",
    "    # Concatenate the original dataframe with the unigram count dataframe and the TF-IDF dataframe\n",
    "    unigram_train, unigram_test = feature_unigram(train, test)\n",
    "    tfidf_train, tfidf_test = feature_Tfidf(train, test)\n",
    "    x_train = pd.concat([unigram_train, tfidf_train], axis=1)\n",
    "    x_test = pd.concat([unigram_test, tfidf_test], axis=1)\n",
    "    # x_train = unigram_train\n",
    "    # x_test = unigram_test\n",
    "\n",
    "    # Convert the labels into binary arrays\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(train['label'])\n",
    "    y_test = mlb.transform(test['label'])\n",
    "    # Create a dataframe for the binary label arrays\n",
    "    y_train = pd.DataFrame(y, columns=mlb.classes_)\n",
    "    y_test = pd.DataFrame(y_test, columns=mlb.classes_)\n",
    "    \n",
    "    # Create an instance of LogisticRegression\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    y_pred = np.zeros(y_test.shape)\n",
    "    # Iterate through each label and train a separate binary classifier\n",
    "    for i in range(y_train.shape[1]):\n",
    "        label = mlb.classes_[i]\n",
    "        print(\"Training classifier for label:\", label)\n",
    "        # Fit the model on the training data for the current label\n",
    "        clf.fit(x_train, y_train.iloc[:, i])\n",
    "        # Predict the probabilities of the current label for the testing data\n",
    "        y_pred_prob = clf.predict_proba(x_test)[:, 1] # Use probabilities of positive class (1)\n",
    "\n",
    "        # Set the threshold for category assignment\n",
    "        threshold = 0.2\n",
    "\n",
    "        # Generate predicted labels based on threshold\n",
    "        y_pred_labels = np.where(y_pred_prob >= threshold, 1, 0)\n",
    "\n",
    "        # Add predicted labels to the corresponding column in the binary label array\n",
    "        y_pred[:, i] = y_pred_labels.tolist()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "    print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "141f228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_expansion(df):\n",
    "    original_labels = df['label'].tolist()\n",
    "    original_polarities = df['polarity'].tolist()\n",
    "    \n",
    "    # ==advanced indexing==\n",
    "    reps = [len(val) for val in df['label']]\n",
    "    df = df.loc[np.repeat(df.index.values, reps)]\n",
    "    \n",
    "    df['label'] = [item for sublist in original_labels for item in sublist]\n",
    "    df['polarity'] = [item for sublist in original_polarities for item in sublist]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def onehotting(x_train, x_test):\n",
    "    # Convert 'label' column in x_train and x_test to one-hot encoding\n",
    "    x_train_onehot = pd.get_dummies(x_train['label'], prefix='class')\n",
    "    x_test_onehot = pd.get_dummies(x_test['label'], prefix='class')\n",
    "\n",
    "    # Get the columns that are missing in x_test_onehot\n",
    "    missing_cols = set(x_train_onehot.columns) - set(x_test_onehot.columns)\n",
    "    for col in missing_cols:\n",
    "        x_test_onehot[col] = 0\n",
    "\n",
    "    # Reorder the columns in x_test_onehot to match the column names in x_train_onehot\n",
    "    x_test_onehot = x_test_onehot[x_train_onehot.columns]\n",
    "    \n",
    "    # Concatenate x_train and x_test with one-hot encoded columns\n",
    "    x_train = pd.concat([x_train, x_train_onehot], axis=1)\n",
    "    x_test = pd.concat([x_test, x_test_onehot], axis=1)\n",
    "\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cec8d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_feature_selection(features='unigram'):\n",
    "    \"\"\"\n",
    "    parameter:\n",
    "        \n",
    "        features: String\n",
    "        which consists of customised features \"unigram,Tfidf,...\" splitted by \",\" \n",
    "    \n",
    "    returns:\n",
    "        x_train, x_test, y_train, y_test = DataFrame\n",
    "        only includes the data needed for training and test.\n",
    "    \"\"\"\n",
    "    Laptops_Train_p1 = parseXML(\"./data/Laptops_Train_p1.xml\")\n",
    "    Laptops_Test_p1_gold = parseXML(\"./data/Laptops_Test_p1_gold.xml\")\n",
    "    train = pd.DataFrame({'text': Laptops_Train_p1['Text'],\n",
    "                          'label': Laptops_Train_p1['Categories'],\n",
    "                          'polarity': Laptops_Train_p1['Polarities']})\n",
    "    test = pd.DataFrame({'text': Laptops_Test_p1_gold['Text'],\n",
    "                         'label': Laptops_Test_p1_gold['Categories'],\n",
    "                         'polarity': Laptops_Test_p1_gold['Polarities']})\n",
    "    # Customised\n",
    "    # Concatenate the original dataframe with the features selected\n",
    "    # Ignore upper/lower case, leading or trailing whitespaces\n",
    "    features_set = {'tfidf','unigram'}\n",
    "    customised_features_set = set(feature.strip().lower() for feature in features.split(\",\"))\n",
    "    if not customised_features_set.issubset(features_set):\n",
    "        raise ValueError(\"Please input with right features.\")\n",
    "\n",
    "    features_list = []\n",
    "    # Appending tuples at first\n",
    "    if 'tfidf' in customised_features_set:\n",
    "        features_list.append(feature_unigram(train, test))\n",
    "    if 'unigram' in customised_features_set:\n",
    "        features_list.append(feature_Tfidf(train, test))\n",
    "    # Unpacking\n",
    "    x_train_features = []\n",
    "    x_test_features = []\n",
    "    for x_train_feature, x_test_feature in features_list:\n",
    "        x_train_features.append(x_train_feature)\n",
    "        x_test_features.append(x_test_feature)\n",
    "    # convert to DataFrame\n",
    "    x_train = pd.concat(x_train_features, axis=1)\n",
    "    x_test = pd.concat(x_test_features, axis=1)\n",
    "    \n",
    "    # Default\n",
    "    # numerical representation of polarity\n",
    "    xy_train = dataframe_expansion(pd.concat([train, x_train], axis=1))\n",
    "    xy_test = dataframe_expansion(pd.concat([test, x_test], axis=1))\n",
    "    y_dict = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "    y_train = xy_train['polarity'].map(y_dict)\n",
    "    y_test = xy_test['polarity'].map(y_dict)\n",
    "    # label one-hotting\n",
    "    x_train, x_test = onehotting(xy_train.drop(['text', 'polarity'], axis=1), xy_test.drop(['text', 'polarity'], axis=1))\n",
    "    # Drop the original 'label' column from x_train and x_test\n",
    "    x_train.drop('label', axis=1, inplace=True)\n",
    "    x_test.drop('label', axis=1, inplace=True)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66e3c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_anaylysis_LogisticRegressionClf(features='unigram'):\n",
    "    \"\"\"\n",
    "        parameter:\n",
    "        \n",
    "        features: String\n",
    "        which consists of customised features \"unigram,Tfidf,...\" splitted by \",\" \n",
    "        and will be used by the training of classifier\n",
    "    \"\"\"\n",
    "    # Feature selection\n",
    "    x_train, x_test, y_train, y_test = sentiment_analysis_feature_selection(features=features)\n",
    "        \n",
    "    # Create an instance of LogisticRegression\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions on training data\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Generate classification report for training data\n",
    "    classification_report_train = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report (Training Data):\\n\", classification_report_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "263ac185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laptops_Train_p1.xml has been parsed. The number of sentences with opinions is 2039(2500).\n",
      "Laptops_Test_p1_gold.xml has been parsed. The number of sentences with opinions is 808(808).\n",
      "Classification Report (Training Data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.68      0.67       274\n",
      "           0       0.33      0.07      0.11        46\n",
      "           1       0.79      0.84      0.81       481\n",
      "\n",
      "    accuracy                           0.74       801\n",
      "   macro avg       0.60      0.53      0.53       801\n",
      "weighted avg       0.72      0.74      0.72       801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_anaylysis_LogisticRegressionClf(features='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048edfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 425.58333400000004,
   "position": {
    "height": "40px",
    "left": "1164.13px",
    "right": "20px",
    "top": "84px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
