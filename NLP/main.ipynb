{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86e0613",
   "metadata": {},
   "source": [
    "# 1. Preprocess - parse XML\n",
    "Use BeautifulSoup to parse XML file. parseXML() is for part-1 only. It outputs with pandas dataframe \"sentence id, sentence, E#A, sentiment\". parseXML_p2() is for part-2 only. It reads review id instead of sentence id and does not read the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df065f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1371924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML(path):\n",
    "    with open(path) as xmldata:\n",
    "        soup = BeautifulSoup(xmldata, \"xml\")\n",
    "    # Create empty lists to store the extracted data\n",
    "    sentence_ids = []\n",
    "    texts = []\n",
    "    categories = []\n",
    "    polarities = []\n",
    "    # Loop through the 'sentence' elements and extract the necessary information\n",
    "    for sentence in soup.find_all('sentence'):\n",
    "        opinions = sentence.find('Opinions')\n",
    "        if opinions is not None:\n",
    "            s_categories = []\n",
    "            s_polarities = []\n",
    "            for opinion in opinions.find_all('Opinion'):\n",
    "                s_categories.append(opinion['category'])\n",
    "                s_polarities.append(opinion['polarity'])\n",
    "            categories.append(s_categories)\n",
    "            polarities.append(s_polarities)\n",
    "        else:\n",
    "            continue\n",
    "        sentence_ids.append(sentence['id'])\n",
    "        texts.append(sentence.find('text').text)           \n",
    "\n",
    "    # Create a pandas dataframe from the extracted data\n",
    "    df = pd.DataFrame({'Sentence ID': sentence_ids,\n",
    "                       'text': texts,\n",
    "                       'label': categories,\n",
    "                       'polarity': polarities})\n",
    "#     print(f'{path.split(\"/\")[-1]} has been parsed. The number of sentences with opinions is '\n",
    "#           f\"{len(soup.find_all('Opinions'))}({len(soup.find_all('sentence'))}).\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52983518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXML_p2(path):\n",
    "    with open(path) as xmldata:\n",
    "        soup = BeautifulSoup(xmldata, \"xml\")\n",
    "    # Create empty lists to store the extracted data\n",
    "    review_rids = []\n",
    "    categories = []\n",
    "    polarities = []\n",
    "    for review in soup.find_all('Review'):\n",
    "        s_categories = []\n",
    "        s_polarities = []\n",
    "        for opinion in review.find_all('Opinion'):\n",
    "            if opinion is not None:\n",
    "                s_categories.append(opinion['category'])\n",
    "                s_polarities.append(opinion['polarity'])\n",
    "        categories.append(s_categories)\n",
    "        polarities.append(s_polarities)\n",
    "        review_rids.append(review['rid'])   \n",
    "            \n",
    "\n",
    "    # Create a pandas dataframe from the extracted data\n",
    "    df = pd.DataFrame({'Review RID': review_rids,\n",
    "                       'label': categories,\n",
    "                       'polarity': polarities})\n",
    "#     print(f'{path.split(\"/\")[-1]} has been parsed. The number of reviews with opinions is '\n",
    "#           f\"{len(soup.find_all('Review'))}({len(df['label'])}).\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f75a54",
   "metadata": {},
   "source": [
    "Get a overview of top n most frequent E\\#A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "158ec316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split categories and create new rows\n",
    "df_copy = parseXML(\"./data/Laptops_Train_p1.xml\").copy()\n",
    "split_categories = df_copy['label'].str.split(',')\n",
    "df_copy = df_copy.assign(Category=split_categories).explode('label')\n",
    "\n",
    "# Get the unique categories and their frequencies\n",
    "category_counts = df_copy['label'].value_counts()\n",
    "top_categories = category_counts.head(10)\n",
    "\n",
    "# Plot the frequencies in descending order\n",
    "top_categories.plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Category Frequencies')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93a6da9",
   "metadata": {},
   "source": [
    "# 2. Part 1 - Sentence-level Aspect Based Sentiment Analysis\n",
    "\n",
    "+ Features - unigrams, tfidf, .etc \n",
    "+ Model - Logistic Regression Classifier\n",
    "+ Evaluation - F-1 score for catagory extraction, accuracy for sentiment analysis\n",
    "\n",
    "## 2.1 Features for Catagory Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee38f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f98aafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    # Tokenize the text using NLTK's word_tokenize function\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if re.sub(r'[^\\w\\s]+', '', word).isalnum()]\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]   \n",
    "    return tokens\n",
    "\n",
    "def feature_unigram(df, df_t):    \n",
    "    # Create a CountVectorizer instance to compute unigram counts\n",
    "    count_vectorizer = CountVectorizer(tokenizer=custom_tokenizer,max_features=1000)\n",
    "    # Fit and transform the sentences to obtain the unigram count features\n",
    "    count_matrix = count_vectorizer.fit_transform(df['text'])\n",
    "    count_matrix_t = count_vectorizer.transform(df_t['text'])\n",
    "    # Convert the count matrix to a dataframe with appropriate column names\n",
    "    count_df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "    count_df_t = pd.DataFrame(count_matrix_t.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "    return count_df, count_df_t\n",
    "\n",
    "def feature_Tfidf(df, df_t):\n",
    "    # Create a TfidfVectorizer instance to compute TF-IDF features\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer)\n",
    "    # Fit and transform the sentences to obtain the TF-IDF features\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'])\n",
    "    tfidf_matrix_t = tfidf_vectorizer.transform(df_t['text'])\n",
    "    # Convert the TF-IDF matrix to a dataframe with appropriate column names\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    tfidf_df_t = pd.DataFrame(tfidf_matrix_t.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    return tfidf_df, tfidf_df_t\n",
    "\n",
    "def catagory_extraction_feature_selection(train, test, features='unigram'):\n",
    "    \"\"\"\n",
    "    parameter:\n",
    "        train,test: df\n",
    "        dataset being trained and tested on\n",
    "        features: String\n",
    "        which consists of customised features \"unigram,Tfidf,...\" splitted by \",\" \n",
    "    \n",
    "    returns:\n",
    "        x_train, x_test, y_train, y_test = DataFrame\n",
    "        only includes the data needed for training and test.\n",
    "    \"\"\"\n",
    "    # Customised\n",
    "    # Concatenate the original dataframe with the features selected\n",
    "    # Ignore upper/lower case, leading or trailing whitespaces\n",
    "    features_set = {'tfidf','unigram'}\n",
    "    customised_features_set = set(feature.strip().lower() for feature in features.split(\",\"))\n",
    "    if not customised_features_set.issubset(features_set):\n",
    "        raise ValueError(\"Please input with right features.\")\n",
    "\n",
    "    features_list = []\n",
    "    # Appending tuples at first\n",
    "    if 'tfidf' in customised_features_set:\n",
    "        features_list.append(feature_unigram(train, test))\n",
    "    if 'unigram' in customised_features_set:\n",
    "        features_list.append(feature_Tfidf(train, test))\n",
    "    # Unpacking\n",
    "    x_train_features = []\n",
    "    x_test_features = []\n",
    "    for x_train_feature, x_test_feature in features_list:\n",
    "        x_train_features.append(x_train_feature)\n",
    "        x_test_features.append(x_test_feature)\n",
    "    # convert to DataFrame\n",
    "    x_train = pd.concat(x_train_features, axis=1)\n",
    "    x_test = pd.concat(x_test_features, axis=1)\n",
    "        \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c58123",
   "metadata": {},
   "source": [
    "## 2.2 Logistic Regression Classifier for Catagory Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafd4f18",
   "metadata": {},
   "source": [
    "+ multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb949a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catagory_extraction_LogisticRegressionClf(threshold=0.2,features='unigram'):\n",
    "    \"\"\"\n",
    "        Train the classifer using selected data, make predictions on training data and \n",
    "        Generate classification report for test data\n",
    "        \n",
    "        parameter:\n",
    "            threshold: float, range from 0 to 1\n",
    "            each sentence can be assigned to multiple classes simultaneously with \n",
    "            probability larger than threshold\n",
    "            \n",
    "            features: String\n",
    "            which consists of customised features \"unigram,Tfidf,...\" splitted by \",\" \n",
    "            and will be used by the training of classifier\n",
    "    \"\"\"\n",
    "    #data preprocessing\n",
    "    Laptops_Train_p1 = parseXML(\"./data/Laptops_Train_p1.xml\")\n",
    "    Laptops_Test_p1_gold = parseXML(\"./data/Laptops_Test_p1_gold.xml\")\n",
    "\n",
    "    x_train, x_test = catagory_extraction_feature_selection(Laptops_Train_p1, Laptops_Test_p1_gold, features=features)\n",
    "    \n",
    "    # Convert the labels into binary arrays\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(Laptops_Train_p1['label'])\n",
    "    y_test = mlb.transform(Laptops_Test_p1_gold['label'])\n",
    "    # Create a dataframe for the binary label arrays\n",
    "    y_train = pd.DataFrame(y, columns=mlb.classes_)\n",
    "    y_test = pd.DataFrame(y_test, columns=mlb.classes_)\n",
    "    \n",
    "    # Create an instance of LogisticRegression\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    y_pred = np.zeros(y_test.shape)\n",
    "    # Iterate through each label and train a separate binary classifier\n",
    "    for i in range(y_train.shape[1]):\n",
    "\n",
    "        # Fit the model on the training data for the current label\n",
    "        clf.fit(x_train, y_train.iloc[:, i])\n",
    "        # Predict the probabilities of the current label for the testing data\n",
    "        y_pred_prob = clf.predict_proba(x_test)[:, 1] # Use probabilities of positive class (1)\n",
    "\n",
    "        # Set the threshold for category assignment\n",
    "        t = threshold\n",
    "\n",
    "        # Generate predicted labels based on threshold\n",
    "        y_pred_labels = np.where(y_pred_prob >= t, 1, 0)\n",
    "\n",
    "        # Add predicted labels to the corresponding column in the binary label array\n",
    "        y_pred[:, i] = y_pred_labels.tolist()\n",
    "\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "        \n",
    "    test_predicted_labels = pd.DataFrame({'label': mlb.inverse_transform(y_pred)})    \n",
    "    return pd.concat([Laptops_Test_p1_gold['Sentence ID'], test_predicted_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1da0f622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinpeng/APP/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:860: UserWarning: unknown class(es) ['BATTERY#DESIGN_FEATURES', 'CPU#GENERAL', 'HARD_DISC#GENERAL', 'HARD_DISC#OPERATION_PERFORMANCE', 'OPTICAL_DRIVES#DESIGN_FEATURES', 'OPTICAL_DRIVES#GENERAL', 'POWER_SUPPLY#GENERAL'] will be ignored\n",
      "  warnings.warn('unknown class(es) {0} will be ignored'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         0\n",
      "           1       0.74      0.74      0.74        19\n",
      "           2       1.00      0.00      0.00         5\n",
      "           3       0.57      0.11      0.18        38\n",
      "           4       1.00      0.00      0.00         3\n",
      "           5       1.00      0.00      0.00         1\n",
      "           6       1.00      0.00      0.00         2\n",
      "           7       1.00      1.00      1.00         0\n",
      "           8       1.00      0.00      0.00        13\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       1.00      0.00      0.00         8\n",
      "          11       0.80      0.20      0.32        20\n",
      "          12       1.00      0.00      0.00         5\n",
      "          13       1.00      1.00      1.00         0\n",
      "          14       1.00      0.00      0.00         2\n",
      "          15       1.00      1.00      1.00         0\n",
      "          16       1.00      0.00      0.00         2\n",
      "          17       1.00      1.00      1.00         0\n",
      "          18       1.00      0.00      0.00         1\n",
      "          19       1.00      1.00      1.00         0\n",
      "          20       1.00      1.00      1.00         0\n",
      "          21       1.00      1.00      1.00         0\n",
      "          22       1.00      0.00      0.00         4\n",
      "          23       1.00      0.00      0.00        13\n",
      "          24       1.00      0.00      0.00         1\n",
      "          25       0.00      0.00      0.00         7\n",
      "          26       1.00      0.00      0.00         2\n",
      "          27       1.00      0.00      0.00         9\n",
      "          28       1.00      0.00      0.00         6\n",
      "          29       1.00      0.00      0.00         6\n",
      "          30       1.00      0.00      0.00        13\n",
      "          31       0.46      0.42      0.44        73\n",
      "          32       0.30      0.95      0.46       158\n",
      "          33       0.67      0.29      0.41        34\n",
      "          34       0.61      0.61      0.61        70\n",
      "          35       1.00      0.17      0.29         6\n",
      "          36       0.50      0.48      0.49        25\n",
      "          37       0.51      0.38      0.44        47\n",
      "          38       0.32      0.13      0.18        46\n",
      "          39       1.00      0.00      0.00         8\n",
      "          40       1.00      0.00      0.00         2\n",
      "          41       1.00      0.00      0.00         4\n",
      "          42       1.00      0.00      0.00         4\n",
      "          43       1.00      0.00      0.00         3\n",
      "          44       1.00      0.00      0.00         2\n",
      "          45       1.00      0.00      0.00        13\n",
      "          46       1.00      0.00      0.00         6\n",
      "          47       1.00      0.00      0.00         3\n",
      "          48       1.00      1.00      1.00         0\n",
      "          49       1.00      1.00      1.00         0\n",
      "          50       1.00      0.17      0.29         6\n",
      "          51       1.00      0.00      0.00         1\n",
      "          52       1.00      1.00      1.00         0\n",
      "          53       1.00      1.00      1.00         0\n",
      "          54       1.00      0.00      0.00         5\n",
      "          55       0.33      0.22      0.27         9\n",
      "          56       1.00      0.00      0.00         2\n",
      "          57       1.00      0.00      0.00         1\n",
      "          58       1.00      0.00      0.00         2\n",
      "          59       1.00      0.00      0.00        15\n",
      "          60       1.00      1.00      1.00         0\n",
      "          61       1.00      0.00      0.00         2\n",
      "          62       1.00      1.00      1.00         0\n",
      "          63       1.00      0.00      0.00         1\n",
      "          64       1.00      1.00      1.00         0\n",
      "          65       1.00      1.00      1.00         0\n",
      "          66       1.00      1.00      1.00         0\n",
      "          67       1.00      1.00      1.00         0\n",
      "          68       1.00      0.00      0.00         1\n",
      "          69       1.00      0.00      0.00         1\n",
      "          70       1.00      0.00      0.00         5\n",
      "          71       1.00      1.00      1.00         0\n",
      "          72       1.00      0.00      0.00         2\n",
      "          73       1.00      0.00      0.00         1\n",
      "          74       1.00      0.00      0.00         1\n",
      "          75       1.00      0.00      0.00         7\n",
      "          76       1.00      1.00      1.00         0\n",
      "          77       1.00      0.00      0.00         2\n",
      "          78       0.42      0.15      0.22        33\n",
      "          79       1.00      0.00      0.00         1\n",
      "          80       1.00      1.00      1.00         0\n",
      "\n",
      "   micro avg       0.38      0.38      0.38       786\n",
      "   macro avg       0.90      0.32      0.33       786\n",
      "weighted avg       0.60      0.38      0.31       786\n",
      " samples avg       0.51      0.58      0.35       786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predicted_labels = catagory_extraction_LogisticRegressionClf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726298ad",
   "metadata": {},
   "source": [
    "## 2.3 Features for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141f228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_expansion(df):\n",
    "    original_labels = df['label'].tolist()\n",
    "    original_polarities = df['polarity'].tolist()\n",
    "    \n",
    "    # ==advanced indexing==\n",
    "    reps = [len(val) for val in df['label']]\n",
    "    df = df.loc[np.repeat(df.index.values, reps)]\n",
    "    \n",
    "    df['label'] = [item for sublist in original_labels for item in sublist]\n",
    "    df['polarity'] = [item for sublist in original_polarities for item in sublist]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def dataframe_undoexpansion(y_pred, test):\n",
    "    y_pred = pd.DataFrame({'polarity': y_pred})\n",
    "    y_pred = inverse_numerical_rep(y_pred['polarity'])\n",
    "    y_pred = y_pred.tolist()\n",
    "    y_aligned = []\n",
    "    for index, row in test.iterrows():\n",
    "        sublist = []\n",
    "        for i in range(len(row['polarity'])):\n",
    "            sublist.append(y_pred.pop(0))\n",
    "        y_aligned.append(sublist)\n",
    "    test_pred = test.copy()\n",
    "    test_pred['polarity'] = y_aligned\n",
    "    return test_pred\n",
    "\n",
    "def onehotting(x_train, x_test):\n",
    "    # Convert 'label' column in x_train and x_test to one-hot encoding\n",
    "    x_train_onehot = pd.get_dummies(x_train['label'], prefix='class')\n",
    "    x_test_onehot = pd.get_dummies(x_test['label'], prefix='class')\n",
    "\n",
    "    # Get the columns that are missing in x_test_onehot\n",
    "    missing_cols = set(x_train_onehot.columns) - set(x_test_onehot.columns)\n",
    "    for col in missing_cols:\n",
    "        x_test_onehot[col] = 0\n",
    "\n",
    "    # Reorder the columns in x_test_onehot to match the column names in x_train_onehot\n",
    "    x_test_onehot = x_test_onehot[x_train_onehot.columns]\n",
    "    \n",
    "    # Concatenate x_train and x_test with one-hot encoded columns\n",
    "    x_train = pd.concat([x_train, x_train_onehot], axis=1)\n",
    "    x_test = pd.concat([x_test, x_test_onehot], axis=1)\n",
    "\n",
    "    return x_train, x_test\n",
    "\n",
    "def numerical_rep(df):\n",
    "    df = df.copy()\n",
    "    y_dict = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "    y = df.map(y_dict)\n",
    "    return y\n",
    "\n",
    "def inverse_numerical_rep(df):\n",
    "    df = df.copy()\n",
    "    y_dict = {'positive': 1, 'negative': -1, 'neutral': 0}\n",
    "    reverse_y_dict = {v: k for k, v in y_dict.items()} # Reverse the keys and values in y_dict\n",
    "    y = df.map(reverse_y_dict)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc8e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_feature_selection(train, test, features='unigram'):\n",
    "    \"\"\"\n",
    "    parameter:\n",
    "        train,test: df\n",
    "        dataset being trained and tested on\n",
    "        features: String\n",
    "        which consists of customised features \"unigram,Tfidf,...\" splitted by \",\" \n",
    "    \n",
    "    returns:\n",
    "        x_train, x_test, y_train, y_test = DataFrame\n",
    "        only includes the data needed for training and test.\n",
    "    \"\"\"\n",
    "    # Customised\n",
    "    # Concatenate the original dataframe with the features selected\n",
    "    # Ignore upper/lower case, leading or trailing whitespaces\n",
    "    features_set = {'tfidf','unigram'}\n",
    "    customised_features_set = set(feature.strip().lower() for feature in features.split(\",\"))\n",
    "    if not customised_features_set.issubset(features_set):\n",
    "        raise ValueError(\"Please input with right features.\")\n",
    "\n",
    "    features_list = []\n",
    "    # Appending tuples at first\n",
    "    if 'tfidf' in customised_features_set:\n",
    "        features_list.append(feature_unigram(train, test))\n",
    "    if 'unigram' in customised_features_set:\n",
    "        features_list.append(feature_Tfidf(train, test))\n",
    "    # Unpacking\n",
    "    x_train_features = []\n",
    "    x_test_features = []\n",
    "    for x_train_feature, x_test_feature in features_list:\n",
    "        x_train_features.append(x_train_feature)\n",
    "        x_test_features.append(x_test_feature)\n",
    "    # convert to DataFrame\n",
    "    x_train = pd.concat(x_train_features, axis=1)\n",
    "    x_test = pd.concat(x_test_features, axis=1)\n",
    "    \n",
    "    # Default\n",
    "    # numerical representation of polarity\n",
    "    xy_train = dataframe_expansion(pd.concat([train, x_train], axis=1))\n",
    "    xy_test = dataframe_expansion(pd.concat([test, x_test], axis=1))\n",
    "    y_train = numerical_rep(xy_train['polarity'])\n",
    "    y_test = numerical_rep(xy_test['polarity'])\n",
    "    # label one-hotting\n",
    "    x_train, x_test = onehotting(xy_train, xy_test)\n",
    "    # Drop the original 'label' column from x_train and x_test\n",
    "    x_train.drop(train.columns.values.tolist(), axis=1, inplace=True)\n",
    "    x_test.drop(test.columns.values.tolist(), axis=1, inplace=True)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d86f6a",
   "metadata": {},
   "source": [
    "## 2.4 Logistic Regression Classifer for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4064b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_anaylysis_LogisticRegressionClf(features='unigram'):\n",
    "    \"\"\"\n",
    "        Train the classifer using selected data, make predictions on training data and \n",
    "        Generate classification report for test data\n",
    "        \n",
    "        parameter:        \n",
    "            features: String\n",
    "            which consists of customised features \"unigram,Tfidf,...\" splitted by \",\" \n",
    "            and will be used by the training of classifier\n",
    "    \"\"\"\n",
    "    Laptops_Train_p1 = parseXML(\"./data/Laptops_Train_p1.xml\")\n",
    "    Laptops_Test_p1_gold = parseXML(\"./data/Laptops_Test_p1_gold.xml\")\n",
    "\n",
    "    # Feature selection\n",
    "    x_train, x_test, y_train, y_test = sentiment_analysis_feature_selection(Laptops_Train_p1, Laptops_Test_p1_gold, features=features)\n",
    "        \n",
    "    # Create an instance of LogisticRegression\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions on training data\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # Generate classification report for training data\n",
    "    classification_report_train = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report (Training Data):\\n\", classification_report_train)\n",
    "    \n",
    "    # Data format alignment\n",
    "    test_aligned = dataframe_undoexpansion(y_pred, Laptops_Test_p1_gold)\n",
    "    \n",
    "    return test_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd67bd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Training Data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.66      0.68      0.67       274\n",
      "           0       0.33      0.07      0.11        46\n",
      "           1       0.79      0.84      0.81       481\n",
      "\n",
      "    accuracy                           0.74       801\n",
      "   macro avg       0.60      0.53      0.53       801\n",
      "weighted avg       0.72      0.74      0.72       801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_aligned = sentiment_anaylysis_LogisticRegressionClf(features='unigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82085c87",
   "metadata": {},
   "source": [
    "# 3. Part 2 - Text-level Aspect Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf54fb",
   "metadata": {},
   "source": [
    "Traverse the predicted sentence-level tuples of the same category and counts the respective polarity labels (positive, negative or neutral). Finally, the polarity label with the highest frequency is assigned to the text-level category. If there are not any sentence-level tuples of the same category the polarity label is determined based on all tuples regardless of the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3deffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efdfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_to_text_level(df):\n",
    "    \n",
    "    df = df.copy()    \n",
    "    # Split the 'Sentence ID' column by ':' and only keep the first part\n",
    "    df['Sentence ID'] = df['Sentence ID'].apply(lambda x: x.split(':')[0])\n",
    "    # Group the rows by 'Sentence ID' and combine the 'label' and 'polarity' values into a list\n",
    "    grouped_df = df.groupby('Sentence ID', sort=False).agg({'label': lambda x: [item for sublist in x for item in sublist]})\n",
    "    # Reset the index to turn 'Sentence ID' back into a regular column\n",
    "    grouped_df = grouped_df.reset_index()\n",
    "    grouped_df = grouped_df.rename(columns={'Sentence ID': 'Review RID'})\n",
    "    \n",
    "    return grouped_df\n",
    "\n",
    "def group_to_text_level2(df):\n",
    "    \n",
    "    df = df.copy()    \n",
    "    # Split the 'Sentence ID' column by ':' and only keep the first part\n",
    "    df['Sentence ID'] = df['Sentence ID'].apply(lambda x: x.split(':')[0])\n",
    "    # Group the rows by 'Sentence ID' and combine the 'label' and 'polarity' values into a list\n",
    "    grouped_df = df.groupby('Sentence ID',sort=False).agg({'label': lambda x: sum(x, []), 'polarity': lambda x: sum(x, [])})\n",
    "    # Reset the index to turn 'Sentence ID' back into a regular column\n",
    "    grouped_df = grouped_df.reset_index()\n",
    "    grouped_df = grouped_df.rename(columns={'Sentence ID': 'Review RID'})\n",
    "    \n",
    "    return grouped_df\n",
    "\n",
    "def get_most_frequent_polarity_dist(row):\n",
    "    dict1 = {}\n",
    "    for key, value in zip(row['label'], row['polarity']):\n",
    "        if key in dict1:\n",
    "            dict1[key].append(value)\n",
    "        else:\n",
    "            dict1[key] = [value]\n",
    "    most_frequent_values = {}\n",
    "    for key, value_list in dict1.items():\n",
    "        most_frequent_values[key] = max(set(value_list), key=value_list.count)\n",
    "\n",
    "    return most_frequent_values\n",
    "    \n",
    "def get_polarities_frequency_dict(row):\n",
    "    frequency_distribution = Counter(row['polarity'])\n",
    "    most_frequent_key = frequency_distribution.most_common(1)[0][0]\n",
    "    return most_frequent_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b37967d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def caculate_accuracy(df):\n",
    "    # Initialize variables for the total number of predictions and correct predictions\n",
    "    total_predictions = 0\n",
    "    total_correct_predictions = 0\n",
    "\n",
    "    # Iterate over each row in the dataframe\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the actual and predicted polarity values as lists of strings\n",
    "        actual_polarity = row['polarity']\n",
    "        predicted_polarity = row['pred_polarity']\n",
    "\n",
    "        # Calculate the number of correctly predicted polarities\n",
    "        num_correct = sum(1 for x, y in zip(actual_polarity, predicted_polarity) if x == y)\n",
    "\n",
    "        # Update the total number of predictions and correct predictions\n",
    "        total_predictions += len(actual_polarity)\n",
    "        total_correct_predictions += num_correct\n",
    "\n",
    "    # Calculate the overall accuracy as a fraction\n",
    "    overall_accuracy = total_correct_predictions / total_predictions\n",
    "    \n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "109aa7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_level_ABSA(test_pred, test_predicted_labels):\n",
    "    \"\"\"\n",
    "        Determine the polarity and label of test-level based on sentence-level results.\n",
    "        The polarity with the highest frequency is assigned to the text-level category.\n",
    "        If there are not any sentence-level tuples of the same category the polarity \n",
    "        label is determined based on all tuples regardless of the category.\n",
    "        \n",
    "        parameter:        \n",
    "            test_pred: df\n",
    "            sentence-level sentiment results\n",
    "    \"\"\"\n",
    "    test_p2 = parseXML_p2(\"./data/Laptops_Test_p2_gold.xml\")\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    test_predicted_labels = group_to_text_level(test_predicted_labels)\n",
    "    y = mlb.fit_transform(test_p2['label'])\n",
    "    y_pred = mlb.transform(test_predicted_labels['label'])\n",
    "    # Create a dataframe for the binary label arrays\n",
    "    y = pd.DataFrame(y, columns=mlb.classes_)\n",
    "    y_pred = pd.DataFrame(y_pred, columns=mlb.classes_)\n",
    "    # Generate classification report for training data\n",
    "    classification_report_train = classification_report(y, y_pred, zero_division=1)\n",
    "    print(\"Classification Report (Training Data):\\n\", classification_report_train)\n",
    "\n",
    "    df = group_to_text_level2(test_pred)\n",
    "    test_p2['most_frequent_polarity_dist'] = df.apply(get_most_frequent_polarity_dist, axis=1)\n",
    "    test_p2['polarity_distribution'] = df.apply(get_polarities_frequency_dict, axis=1)\n",
    "    \n",
    "    test_p2 = test_p2.reset_index()\n",
    "    text_p2_pred = []\n",
    "    for index, row in test_p2.iterrows():\n",
    "        sublist = []\n",
    "        for label in row['label']:\n",
    "            if label in row['most_frequent_polarity_dist']:\n",
    "                sublist.append(row['most_frequent_polarity_dist'][label])\n",
    "            else:\n",
    "                sublist.append(row['polarity_distribution'])\n",
    "        text_p2_pred.append(sublist)\n",
    "    test_p2['pred_polarity'] = text_p2_pred\n",
    "    \n",
    "    print(\"Overall accuracy of sentiment polarity of text-level E#A: {:.2%}\".format(caculate_accuracy(test_p2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "703780bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report (Training Data):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00         1\n",
      "           1       0.86      0.86      0.86        14\n",
      "           2       1.00      0.00      0.00         3\n",
      "           3       0.71      0.21      0.32        24\n",
      "           4       1.00      0.00      0.00         3\n",
      "           5       1.00      0.00      0.00         1\n",
      "           6       1.00      0.00      0.00         1\n",
      "           7       1.00      0.00      0.00         3\n",
      "           8       1.00      0.00      0.00        12\n",
      "           9       0.00      0.00      0.00         4\n",
      "          10       1.00      0.00      0.00         6\n",
      "          11       0.80      0.19      0.31        21\n",
      "          12       1.00      0.00      0.00         5\n",
      "          13       1.00      0.00      0.00         2\n",
      "          14       1.00      0.00      0.00         2\n",
      "          15       1.00      0.00      0.00         1\n",
      "          16       1.00      0.00      0.00         3\n",
      "          17       1.00      0.00      0.00         9\n",
      "          18       1.00      0.00      0.00         2\n",
      "          19       1.00      0.00      0.00         3\n",
      "          20       1.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         8\n",
      "          22       1.00      0.00      0.00         2\n",
      "          23       1.00      0.00      0.00         4\n",
      "          24       1.00      0.00      0.00         5\n",
      "          25       1.00      0.00      0.00         6\n",
      "          26       1.00      0.00      0.00         7\n",
      "          27       0.79      0.69      0.74        39\n",
      "          28       1.00      1.00      1.00        80\n",
      "          29       0.77      0.42      0.54        24\n",
      "          30       0.84      0.80      0.82        46\n",
      "          31       1.00      0.20      0.33         5\n",
      "          32       0.72      0.48      0.58        27\n",
      "          33       0.65      0.59      0.62        29\n",
      "          34       0.69      0.37      0.48        30\n",
      "          35       1.00      0.00      0.00         7\n",
      "          36       1.00      0.00      0.00         2\n",
      "          37       1.00      0.00      0.00         4\n",
      "          38       1.00      0.00      0.00         3\n",
      "          39       1.00      0.00      0.00         2\n",
      "          40       1.00      0.00      0.00         2\n",
      "          41       1.00      0.00      0.00         9\n",
      "          42       1.00      0.00      0.00         2\n",
      "          43       1.00      0.00      0.00         3\n",
      "          44       1.00      0.25      0.40         4\n",
      "          45       1.00      0.00      0.00         1\n",
      "          46       1.00      0.00      0.00         1\n",
      "          47       1.00      0.00      0.00         1\n",
      "          48       1.00      0.00      0.00         5\n",
      "          49       0.40      0.22      0.29         9\n",
      "          50       1.00      0.00      0.00         2\n",
      "          51       1.00      0.00      0.00         2\n",
      "          52       1.00      0.00      0.00         2\n",
      "          53       1.00      0.00      0.00        15\n",
      "          54       1.00      0.00      0.00         1\n",
      "          55       1.00      0.00      0.00         2\n",
      "          56       1.00      0.00      0.00         1\n",
      "          57       1.00      0.00      0.00         1\n",
      "          58       1.00      0.00      0.00         1\n",
      "          59       1.00      0.00      0.00         4\n",
      "          60       1.00      0.00      0.00         2\n",
      "          61       1.00      0.00      0.00         1\n",
      "          62       1.00      0.00      0.00         2\n",
      "          63       1.00      0.00      0.00         6\n",
      "          64       1.00      0.00      0.00         2\n",
      "          65       0.67      0.36      0.47        11\n",
      "          66       1.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.82      0.41      0.55       544\n",
      "   macro avg       0.92      0.10      0.12       544\n",
      "weighted avg       0.85      0.41      0.45       544\n",
      " samples avg       0.84      0.44      0.55       544\n",
      "\n",
      "Overall accuracy of sentiment polarity of text-level E#A: 73.58%\n"
     ]
    }
   ],
   "source": [
    "text_level_ABSA(test_aligned,test_predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5686a6",
   "metadata": {},
   "source": [
    "# 4. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99131976",
   "metadata": {},
   "source": [
    "**Evaluation**:\n",
    "+ part-1 sentence-level ABSA\n",
    " + catagory extraction - micro F1-score: 0.38\n",
    " + sentiment analysis - accuracy: 0.74\n",
    "+ part-2 text-level ABSA\n",
    " + catagory extraction - micro F1-score:0.55\n",
    " + sentiment analysis - accuracy: 0.74\n",
    "\n",
    "**Future things to try**:\n",
    "+ Instead of using tf-idf or unigram features, apply word2vec/GloVe/FastText/context2vec  embedding techniques trained on external dataset to capture the syntactic and semantic relationships between words in a sentence.\n",
    "+ Instead of using logistic regression for classification, apply Pre-trained Language Models(PLM) such as BERT or OpenGPT, add specific layers on top of the PLM to generate the final output for the classification task, and then train it alone with the fixed PLM. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 425.58333400000004,
   "position": {
    "height": "40px",
    "left": "1164.13px",
    "right": "20px",
    "top": "84px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
