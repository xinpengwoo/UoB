{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Natural Language Processing</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-1: Use the below given text and strip off any affixes:\n",
    "   \n",
    "DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive   power derives from a mandate from the masses, not from some farcical aquatic ceremony. The striped bats are hanging on their feet for best.\n",
    "\n",
    "Use the following stemmers available in [NLTK](https://www.nltk.org/) to perform stemming of the above given text and make a comparison of their output:\n",
    "\n",
    "- [Porter stemmer](https://www.nltk.org/api/nltk.stem.porter.html) \n",
    "- [Lancaster stemmer](https://www.nltk.org/api/nltk.stem.lancaster.html)\n",
    "- [Snowball stemmer](https://www.nltk.org/api/nltk.stem.snowball.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stemmer:\n",
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.', 'the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best', '.']\n",
      "Lancaster stemmer:\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.', 'the', 'striped', 'bat', 'ar', 'hang', 'on', 'their', 'feet', 'for', 'best', '.']\n",
      "Snowball stemmer:\n",
      "denni : listen , strang women lie in pond distribut sword is no basi for a system of govern . suprem execut power deriv from a mandat from the mass , not from some farcic aquat ceremoni . the stripe bat are hang on their feet for best .\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "from nltk.tokenize import word_tokenize\n",
    "string = \"\"\"\n",
    "DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony. The striped bats are hanging on their feet for best.\n",
    "\"\"\"\n",
    "stringT = word_tokenize(string.lower())\n",
    "\n",
    "stemmer = stem.PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in stringT]\n",
    "print(\"Porter stemmer:\")\n",
    "print(stemmed)\n",
    "\n",
    "stemmer2 = stem.LancasterStemmer()\n",
    "stemmed2 = [stemmer2.stem(word) for word in stringT]\n",
    "print(\"Lancaster stemmer:\")\n",
    "print(stemmed2)\n",
    "\n",
    "stemmer3 = stem.SnowballStemmer(\"english\")\n",
    "stemmed3 = [stemmer3.stem(word) for word in stringT]\n",
    "print(\"Snowball stemmer:\")\n",
    "print(\" \".join(stemmed3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-1: By using an [NLTK lemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html), such as the wordnet lemmatizer, perform the lemmatization of the following text and compare the lematized text with the text produced with Porter stemmer:\n",
    "\n",
    "DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony. The striped bats are hanging on their feet for best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordnet lemmatizer:\n",
      "['dennis', ':', 'listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.', 'the', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best', '.']\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "lematized = [wnl.lemmatize(word) for word in stringT]\n",
    "print(\"Wordnet lemmatizer:\")\n",
    "print(lematized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task-1: Develop an information retrieval system based on ranked retrieval. The intended system should be based on tf-idf scores and cosine similarities to retrieve ranked indices of documents most relevant to the need. \n",
    "\n",
    "\n",
    "A collection of documents ([WordsDataset.csv](https://canvas.bham.ac.uk/courses/65790/files/14306214?module_item_id=3017551)) and a set of [queries](https://canvas.bham.ac.uk/courses/65790/files/14306235?module_item_id=3017553) are available in the course folder to develop the desired system. This is a sample dataset where every document is a collection of a few words.\n",
    "\n",
    "Upon querying, the query should be compared to the words of every document based on the mentioned scheme and returns ranked (sorted top 10 highest) indices most relevant to the query.\n",
    "\n",
    "Hint: Go to the [lecture slides](https://canvas.bham.ac.uk/courses/65790/files/14297069?module_item_id=3015094) and follow the steps to develop an end-to-end IR system\n",
    "\n",
    "Useful links:  [NLTK](https://www.nltk.org/) [pandas](https://pandas.pydata.org/docs/user_guide/index.html), [NumPy](https://numpy.org/doc/stable/user/index.html#user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Hiker, demon, creepy, scary, tunnel, stalk', 1: 'Batman, batman beyond, who are you, narrows it down, animated, show, officer', 2: 'Up, carl, russell, honor, award, scout badge, old man, kids, movie, record', 3: 'Tom, jerry, sword, stab, dont care, cartoon, show', 4: 'Wholesome, comic, dialogue bubble, dog, sleeping with owner', 5: 'Doug dimmadome, chef hat, long, fast food, restaurant, employee', 6: 'Empty town, comparison, bustling city, contradictory', 7: 'Lord of the rings, lotr, gandalf, pipip, sending, movie, height', 8: 'Geralt, yennefer, pointing, blame, slapstick, video game, player', 9: 'Goofy, college, max ,shock, surprise, reveal, announce, reaction, cartoon', 10: 'Gordon Ramsay, pepto bismol, patrick, feeding, crossover, cartoon, show, chef', 11: 'Groot, gunpoint, force, surreal, movie, despicable me', 12: 'Having enough, jump, slapstick, fall', 13: 'Cat, possessed', 14: 'Hotdog, dog, many options', 15: 'Jedi, master, lightsaber, block, unexpected, movie, star wars', 16: 'Joker, you think this is funny, laugh, reaction, movie', 17: 'Dark, kids, swing, police, dead body', 18: 'Kylo ren, lightsaber, fight, jesus, comparison, movie, starwars', 19: 'Lord of the rings, lotr, cast it into fire, destroy it, movie', 20: \"Master of forbidden knowledge, anime, contradict god's teaching\", 21: 'Mike wazowski, face swap, monster inc, stone face, reaction, movie', 22: 'Mr. Moseby, sweet life of zach and cody, stereotype play, 4 step, show', 23: 'Ninja turtle, growing, old , support, classic, cartoon, show', 24: 'Perry, ferb, Phineas, looking at both, at the same time, show, cartoon', 25: 'Proud, mabel, mark soos, scaring, slapstick', 26: 'Serious guy, ride, unexpected', 27: 'Meet the pyro, comparison, contradictory, videogame, team fortress 2', 28: 'Stupid intelligence, brain, what is this answer', 29: 'Terminator, carrying, mr.bean, on sofa, holding broom', 30: 'The crusade knows no bounds, underwater, diver, sword', 31: 'Titans, sea, battle', 32: 'Toy, biting, cat', 33: 'Two guys, split, slide, combine', 34: 'Undertaker, randy Orton, surprised, reaction, wwe', 35: 'Uno, draw 25, option, classic', 36: 'Muscle arm, hand shake, alone', 37: 'Wolverine, cartoon, show third wheel, left out, reaction', 38: 'Bear, no change, same, fool, cardboard cutout', 39: 'Keanu Reeves, big, small, bench, sad, wholesome', 40: 'Black bear, white bear, panda, combine, fusion, dragonballZ', 41: 'Prefer water, dislike drink, girl', 42: 'Incredibles, frozone, police, i said freeze, excuse, movie, animated', 43: 'Peter Parker, spider man, joker, callmecarson, dancing, crying, crossover', 44: 'Putin, political, reaction, surprised', 45: 'SpongeBob, tear, paper truth', 46: 'Charlie, phone, reaction, crying', 47: 'Fallout, puncher, bigger, regret, cartoon', 48: \"Vacuum, can't fit, wall, surreal\", 49: 'Girl, sign, creative, empty', 50: 'Harry Osborn, peter parker, spider man, movie, hidden, voeyer', 51: 'Jesus, blessing, woman, better', 52: 'Women Smiling, kick, surprised, slapstick', 53: 'video game, football, harry kane, umpire, sport'}\n",
      "{0: '25 batman alone man', 1: 'lack of intelligence', 2: 'game of soccer', 3: 'undertaker wwe record', 4: 'movie for kids', 5: 'harry kane height'}\n",
      "{0: ['hiker', ',', 'demon', ',', 'creepy', ',', 'scary', ',', 'tunnel', ',', 'stalk'], 1: ['batman', ',', 'batman', 'beyond', ',', 'who', 'are', 'you', ',', 'narrows', 'it', 'down', ',', 'animated', ',', 'show', ',', 'officer'], 2: ['up', ',', 'carl', ',', 'russell', ',', 'honor', ',', 'award', ',', 'scout', 'badge', ',', 'old', 'man', ',', 'kids', ',', 'movie', ',', 'record'], 3: ['tom', ',', 'jerry', ',', 'sword', ',', 'stab', ',', 'dont', 'care', ',', 'cartoon', ',', 'show'], 4: ['wholesome', ',', 'comic', ',', 'dialogue', 'bubble', ',', 'dog', ',', 'sleeping', 'with', 'owner'], 5: ['doug', 'dimmadome', ',', 'chef', 'hat', ',', 'long', ',', 'fast', 'food', ',', 'restaurant', ',', 'employee'], 6: ['empty', 'town', ',', 'comparison', ',', 'bustling', 'city', ',', 'contradictory'], 7: ['lord', 'of', 'the', 'rings', ',', 'lotr', ',', 'gandalf', ',', 'pipip', ',', 'sending', ',', 'movie', ',', 'height'], 8: ['geralt', ',', 'yennefer', ',', 'pointing', ',', 'blame', ',', 'slapstick', ',', 'video', 'game', ',', 'player'], 9: ['goofy', ',', 'college', ',', 'max', ',', 'shock', ',', 'surprise', ',', 'reveal', ',', 'announce', ',', 'reaction', ',', 'cartoon'], 10: ['gordon', 'ramsay', ',', 'pepto', 'bismol', ',', 'patrick', ',', 'feeding', ',', 'crossover', ',', 'cartoon', ',', 'show', ',', 'chef'], 11: ['groot', ',', 'gunpoint', ',', 'force', ',', 'surreal', ',', 'movie', ',', 'despicable', 'me'], 12: ['having', 'enough', ',', 'jump', ',', 'slapstick', ',', 'fall'], 13: ['cat', ',', 'possessed'], 14: ['hotdog', ',', 'dog', ',', 'many', 'options'], 15: ['jedi', ',', 'master', ',', 'lightsaber', ',', 'block', ',', 'unexpected', ',', 'movie', ',', 'star', 'wars'], 16: ['joker', ',', 'you', 'think', 'this', 'is', 'funny', ',', 'laugh', ',', 'reaction', ',', 'movie'], 17: ['dark', ',', 'kids', ',', 'swing', ',', 'police', ',', 'dead', 'body'], 18: ['kylo', 'ren', ',', 'lightsaber', ',', 'fight', ',', 'jesus', ',', 'comparison', ',', 'movie', ',', 'starwars'], 19: ['lord', 'of', 'the', 'rings', ',', 'lotr', ',', 'cast', 'it', 'into', 'fire', ',', 'destroy', 'it', ',', 'movie'], 20: ['master', 'of', 'forbidden', 'knowledge', ',', 'anime', ',', 'contradict', 'god', \"'s\", 'teaching'], 21: ['mike', 'wazowski', ',', 'face', 'swap', ',', 'monster', 'inc', ',', 'stone', 'face', ',', 'reaction', ',', 'movie'], 22: ['mr.', 'moseby', ',', 'sweet', 'life', 'of', 'zach', 'and', 'cody', ',', 'stereotype', 'play', ',', '4', 'step', ',', 'show'], 23: ['ninja', 'turtle', ',', 'growing', ',', 'old', ',', 'support', ',', 'classic', ',', 'cartoon', ',', 'show'], 24: ['perry', ',', 'ferb', ',', 'phineas', ',', 'looking', 'at', 'both', ',', 'at', 'the', 'same', 'time', ',', 'show', ',', 'cartoon'], 25: ['proud', ',', 'mabel', ',', 'mark', 'soos', ',', 'scaring', ',', 'slapstick'], 26: ['serious', 'guy', ',', 'ride', ',', 'unexpected'], 27: ['meet', 'the', 'pyro', ',', 'comparison', ',', 'contradictory', ',', 'videogame', ',', 'team', 'fortress', '2'], 28: ['stupid', 'intelligence', ',', 'brain', ',', 'what', 'is', 'this', 'answer'], 29: ['terminator', ',', 'carrying', ',', 'mr.bean', ',', 'on', 'sofa', ',', 'holding', 'broom'], 30: ['the', 'crusade', 'knows', 'no', 'bounds', ',', 'underwater', ',', 'diver', ',', 'sword'], 31: ['titans', ',', 'sea', ',', 'battle'], 32: ['toy', ',', 'biting', ',', 'cat'], 33: ['two', 'guys', ',', 'split', ',', 'slide', ',', 'combine'], 34: ['undertaker', ',', 'randy', 'orton', ',', 'surprised', ',', 'reaction', ',', 'wwe'], 35: ['uno', ',', 'draw', '25', ',', 'option', ',', 'classic'], 36: ['muscle', 'arm', ',', 'hand', 'shake', ',', 'alone'], 37: ['wolverine', ',', 'cartoon', ',', 'show', 'third', 'wheel', ',', 'left', 'out', ',', 'reaction'], 38: ['bear', ',', 'no', 'change', ',', 'same', ',', 'fool', ',', 'cardboard', 'cutout'], 39: ['keanu', 'reeves', ',', 'big', ',', 'small', ',', 'bench', ',', 'sad', ',', 'wholesome'], 40: ['black', 'bear', ',', 'white', 'bear', ',', 'panda', ',', 'combine', ',', 'fusion', ',', 'dragonballz'], 41: ['prefer', 'water', ',', 'dislike', 'drink', ',', 'girl'], 42: ['incredibles', ',', 'frozone', ',', 'police', ',', 'i', 'said', 'freeze', ',', 'excuse', ',', 'movie', ',', 'animated'], 43: ['peter', 'parker', ',', 'spider', 'man', ',', 'joker', ',', 'callmecarson', ',', 'dancing', ',', 'crying', ',', 'crossover'], 44: ['putin', ',', 'political', ',', 'reaction', ',', 'surprised'], 45: ['spongebob', ',', 'tear', ',', 'paper', 'truth'], 46: ['charlie', ',', 'phone', ',', 'reaction', ',', 'crying'], 47: ['fallout', ',', 'puncher', ',', 'bigger', ',', 'regret', ',', 'cartoon'], 48: ['vacuum', ',', 'ca', \"n't\", 'fit', ',', 'wall', ',', 'surreal'], 49: ['girl', ',', 'sign', ',', 'creative', ',', 'empty'], 50: ['harry', 'osborn', ',', 'peter', 'parker', ',', 'spider', 'man', ',', 'movie', ',', 'hidden', ',', 'voeyer'], 51: ['jesus', ',', 'blessing', ',', 'woman', ',', 'better'], 52: ['women', 'smiling', ',', 'kick', ',', 'surprised', ',', 'slapstick'], 53: ['video', 'game', ',', 'football', ',', 'harry', 'kane', ',', 'umpire', ',', 'sport']}\n",
      "{0: ['25', 'batman', 'alone', 'man'], 1: ['lack', 'of', 'intelligence'], 2: ['game', 'of', 'soccer'], 3: ['undertaker', 'wwe', 'record'], 4: ['movie', 'for', 'kids'], 5: ['harry', 'kane', 'height']}\n"
     ]
    }
   ],
   "source": [
    "# Step2: Preprocessing\n",
    "# Convert csv and txt into dict using pandas\n",
    "import pandas as pd\n",
    "documents = pd.read_csv('WordsDataset.csv', header=0, index_col=0, squeeze = True).to_dict()\n",
    "queries = pd.read_csv(\"Queries.txt\", header = None).to_dict()[0]\n",
    "# Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(documents)\n",
    "print(queries)\n",
    "for key, value in documents.items():\n",
    "    documents[key] = word_tokenize(documents[key].lower())\n",
    "for key, value in queries.items():\n",
    "    queries[key] = word_tokenize(queries[key].lower())\n",
    "print(documents)\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
      "{0: ['hiker', 'demon', 'creepi', 'scari', 'tunnel', 'stalk'], 1: ['batman', 'batman', 'beyond', 'narrow', 'anim', 'show', 'offic'], 2: ['carl', 'russel', 'honor', 'award', 'scout', 'badg', 'old', 'man', 'kid', 'movi', 'record'], 3: ['tom', 'jerri', 'sword', 'stab', 'dont', 'care', 'cartoon', 'show'], 4: ['wholesom', 'comic', 'dialogu', 'bubbl', 'dog', 'sleep', 'owner'], 5: ['doug', 'dimmadom', 'chef', 'hat', 'long', 'fast', 'food', 'restaur', 'employ'], 6: ['empti', 'town', 'comparison', 'bustl', 'citi', 'contradictori'], 7: ['lord', 'ring', 'lotr', 'gandalf', 'pipip', 'send', 'movi', 'height'], 8: ['geralt', 'yennef', 'point', 'blame', 'slapstick', 'video', 'game', 'player'], 9: ['goofi', 'colleg', 'max', 'shock', 'surpri', 'reveal', 'announc', 'reaction', 'cartoon'], 10: ['gordon', 'ramsay', 'pepto', 'bismol', 'patrick', 'feed', 'crossov', 'cartoon', 'show', 'chef'], 11: ['groot', 'gunpoint', 'forc', 'surreal', 'movi', 'despic'], 12: ['enough', 'jump', 'slapstick', 'fall'], 13: ['cat', 'possess'], 14: ['hotdog', 'dog', 'mani', 'option'], 15: ['jedi', 'master', 'lightsab', 'block', 'unexpect', 'movi', 'star', 'war'], 16: ['joker', 'think', 'funni', 'laugh', 'reaction', 'movi'], 17: ['dark', 'kid', 'swing', 'polic', 'dead', 'bodi'], 18: ['kylo', 'ren', 'lightsab', 'fight', 'jesu', 'comparison', 'movi', 'starwar'], 19: ['lord', 'ring', 'lotr', 'cast', 'fire', 'destroy', 'movi'], 20: ['master', 'forbidden', 'knowledg', 'anim', 'contradict', 'god', \"'s\", 'teach'], 21: ['mike', 'wazowski', 'face', 'swap', 'monster', 'inc', 'stone', 'face', 'reaction', 'movi'], 22: ['mr.', 'mosebi', 'sweet', 'life', 'zach', 'codi', 'stereotyp', 'play', '4', 'step', 'show'], 23: ['ninja', 'turtl', 'grow', 'old', 'support', 'classic', 'cartoon', 'show'], 24: ['perri', 'ferb', 'phinea', 'look', 'time', 'show', 'cartoon'], 25: ['proud', 'mabel', 'mark', 'soo', 'scare', 'slapstick'], 26: ['seriou', 'guy', 'ride', 'unexpect'], 27: ['meet', 'pyro', 'comparison', 'contradictori', 'videogam', 'team', 'fortress', '2'], 28: ['stupid', 'intellig', 'brain', 'answer'], 29: ['termin', 'carri', 'mr.bean', 'sofa', 'hold', 'broom'], 30: ['crusad', 'know', 'bound', 'underwat', 'diver', 'sword'], 31: ['titan', 'sea', 'battl'], 32: ['toy', 'bite', 'cat'], 33: ['two', 'guy', 'split', 'slide', 'combin'], 34: ['undertak', 'randi', 'orton', 'surpri', 'reaction', 'wwe'], 35: ['uno', 'draw', '25', 'option', 'classic'], 36: ['muscl', 'arm', 'hand', 'shake', 'alon'], 37: ['wolverin', 'cartoon', 'show', 'third', 'wheel', 'left', 'reaction'], 38: ['bear', 'chang', 'fool', 'cardboard', 'cutout'], 39: ['keanu', 'reev', 'big', 'small', 'bench', 'sad', 'wholesom'], 40: ['black', 'bear', 'white', 'bear', 'panda', 'combin', 'fusion', 'dragonballz'], 41: ['prefer', 'water', 'dislik', 'drink', 'girl'], 42: ['incr', 'frozon', 'polic', 'said', 'freez', 'excu', 'movi', 'anim'], 43: ['peter', 'parker', 'spider', 'man', 'joker', 'callmecarson', 'danc', 'cri', 'crossov'], 44: ['putin', 'polit', 'reaction', 'surpri'], 45: ['spongebob', 'tear', 'paper', 'truth'], 46: ['charli', 'phone', 'reaction', 'cri'], 47: ['fallout', 'puncher', 'bigger', 'regret', 'cartoon'], 48: ['vacuum', 'ca', \"n't\", 'fit', 'wall', 'surreal'], 49: ['girl', 'sign', 'creativ', 'empti'], 50: ['harri', 'osborn', 'peter', 'parker', 'spider', 'man', 'movi', 'hidden', 'voeyer'], 51: ['jesu', 'bless', 'woman', 'better'], 52: ['women', 'smile', 'kick', 'surpri', 'slapstick'], 53: ['video', 'game', 'footbal', 'harri', 'kane', 'umpir', 'sport']}\n"
     ]
    }
   ],
   "source": [
    "# Step3: Relevant Information for IR\n",
    "# Remove stopwords and punctuation marks\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation as punc\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "for p in punc:\n",
    "    nltk_stopwords.append(p)\n",
    "print(nltk_stopwords)\n",
    "\n",
    "for key, value in documents.items():\n",
    "    documents[key] = [word for word in documents[key] if not word in nltk_stopwords]\n",
    "for key, value in queries.items():\n",
    "    queries[key] = [word for word in queries[key] if not word in nltk_stopwords]\n",
    "    \n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movi': 0, 'third': 1, 'black': 2, 'crusad': 3, 'groot': 4, 'shake': 5, 'puncher': 6, 'peter': 7, 'fortress': 8, 'team': 9, 'enough': 10, 'gordon': 11, 'option': 12, 'incr': 13, 'comparison': 14, 'water': 15, 'cat': 16, 'step': 17, 'stalk': 18, 'wwe': 19, 'lord': 20, 'zach': 21, 'employ': 22, 'pipip': 23, 'support': 24, 'phinea': 25, 'kick': 26, 'videogam': 27, 'guy': 28, 'hold': 29, 'termin': 30, 'danc': 31, 'patrick': 32, 'kane': 33, 'stereotyp': 34, 'sofa': 35, 'feed': 36, 'swap': 37, 'batman': 38, 'award': 39, 'yennef': 40, 'block': 41, 'sport': 42, 'fallout': 43, 'mr.': 44, 'jesu': 45, 'putin': 46, 'stab': 47, 'fire': 48, 'mark': 49, 'surpri': 50, 'intellig': 51, 'hidden': 52, 'look': 53, 'slide': 54, 'battl': 55, 'bodi': 56, 'sign': 57, 'osborn': 58, 'ferb': 59, 'footbal': 60, 'carri': 61, 'gandalf': 62, 'time': 63, 'polic': 64, 'frozon': 65, 'stone': 66, 'umpir': 67, 'blame': 68, 'dont': 69, 'turtl': 70, 'tear': 71, 'send': 72, 'alon': 73, 'better': 74, 'cast': 75, 'jedi': 76, 'dimmadom': 77, 'sleep': 78, 'gunpoint': 79, 'surreal': 80, 'swing': 81, 'face': 82, 'stupid': 83, 'fit': 84, 'dead': 85, 'sword': 86, 'codi': 87, 'cardboard': 88, 'geralt': 89, 'excu': 90, 'starwar': 91, 'dialogu': 92, 'despic': 93, 'creativ': 94, 'bear': 95, 'possess': 96, 'proud': 97, 'ca': 98, 'inc': 99, 'grow': 100, 'owner': 101, 'contradictori': 102, 'chef': 103, 'lotr': 104, 'monster': 105, 'empti': 106, 'hiker': 107, 'crossov': 108, 'play': 109, 'anim': 110, 'goofi': 111, 'left': 112, 'funni': 113, 'dislik': 114, 'honor': 115, 'smile': 116, 'meet': 117, 'fight': 118, 'parker': 119, 'video': 120, 'randi': 121, 'ren': 122, 'uno': 123, 'food': 124, 'man': 125, 'bismol': 126, 'unexpect': 127, 'perri': 128, 'kid': 129, 'reev': 130, 'drink': 131, 'hotdog': 132, 'white': 133, \"n't\": 134, 'titan': 135, 'ninja': 136, 'offic': 137, 'mike': 138, 'soo': 139, 'game': 140, 'russel': 141, 'know': 142, 'keanu': 143, 'girl': 144, 'bite': 145, 'orton': 146, 'sweet': 147, 'harri': 148, 'fusion': 149, 'war': 150, 'dragonballz': 151, 'tunnel': 152, 'comic': 153, 'mr.bean': 154, 'laugh': 155, 'wolverin': 156, 'joker': 157, 'ring': 158, 'dark': 159, 'old': 160, 'fast': 161, \"'s\": 162, 'broom': 163, 'citi': 164, 'care': 165, 'chang': 166, 'mabel': 167, 'bustl': 168, 'creepi': 169, 'pyro': 170, 'jump': 171, 'toy': 172, 'spider': 173, 'women': 174, 'shock': 175, 'answer': 176, 'demon': 177, 'god': 178, 'fall': 179, 'reveal': 180, 'cri': 181, 'cutout': 182, 'bless': 183, 'seriou': 184, 'cartoon': 185, 'wheel': 186, 'teach': 187, 'phone': 188, 'undertak': 189, 'show': 190, 'combin': 191, 'big': 192, 'forbidden': 193, 'hand': 194, 'lightsab': 195, 'bound': 196, 'muscl': 197, 'freez': 198, 'lack': 199, 'town': 200, 'scout': 201, 'scare': 202, 'brain': 203, 'two': 204, 'point': 205, 'player': 206, 'narrow': 207, 'tom': 208, 'announc': 209, 'callmecarson': 210, 'height': 211, 'fool': 212, 'charli': 213, 'wholesom': 214, 'badg': 215, 'star': 216, 'colleg': 217, 'pepto': 218, 'wazowski': 219, '2': 220, 'reaction': 221, 'long': 222, 'vacuum': 223, 'split': 224, 'restaur': 225, 'forc': 226, 'think': 227, 'regret': 228, 'max': 229, 'prefer': 230, 'life': 231, 'voeyer': 232, 'sea': 233, 'knowledg': 234, 'polit': 235, 'wall': 236, 'underwat': 237, 'beyond': 238, 'jerri': 239, 'sad': 240, 'carl': 241, 'paper': 242, 'contradict': 243, 'small': 244, 'doug': 245, 'panda': 246, 'record': 247, 'spongebob': 248, 'ride': 249, 'kylo': 250, 'said': 251, 'soccer': 252, 'destroy': 253, 'ramsay': 254, '25': 255, 'draw': 256, 'bench': 257, 'diver': 258, 'arm': 259, 'hat': 260, 'mosebi': 261, '4': 262, 'bubbl': 263, 'scari': 264, 'slapstick': 265, 'master': 266, 'mani': 267, 'woman': 268, 'bigger': 269, 'truth': 270, 'dog': 271, 'classic': 272}\n"
     ]
    }
   ],
   "source": [
    "# Reduce dimensionality - stem\n",
    "from nltk import stem\n",
    "stemmer = stem.PorterStemmer()\n",
    "\n",
    "for key, value in documents.items():\n",
    "    stemmed = [stemmer.stem(word) for word in documents[key]]\n",
    "    documents[key] = stemmed\n",
    "for key, value in queries.items():\n",
    "    stemmed = [stemmer.stem(word) for word in queries[key]]\n",
    "    queries[key] = stemmed\n",
    "    \n",
    "# construct termSet and convert it to map\n",
    "termSet = set()\n",
    "for key, value in documents.items():\n",
    "    termSet.update(documents[key])\n",
    "for key, value in queries.items():\n",
    "    termSet.update(queries[key])\n",
    "\n",
    "termMap = {k: v for k, v in zip(termSet, range(len(termSet)))}\n",
    "print(termMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the vector space\n",
    "import numpy as np\n",
    "docTF = np.zeros((len(documents), len(termMap)))\n",
    "queTF = np.zeros((len(queries), len(termMap)))\n",
    "\n",
    "# Construct vector space\n",
    "for key, value in documents.items():\n",
    "    for term in value:\n",
    "        docTF[key,termMap.get(term)] += 1\n",
    "for key, value in queries.items():\n",
    "     for term in value:\n",
    "        queTF[key,termMap.get(term)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.36007936 0.1216192  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.23899627\n",
      "  0.23131359 0.         0.         0.         0.         0.\n",
      "  0.         0.1367546  0.         0.         0.         0.\n",
      "  0.         0.         0.13906793 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.32955732 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.21214754 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.22508434]\n",
      " [0.         0.         0.18397457 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5074669  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.35890429 0.         0.         0.\n",
      "  0.         0.14369148 0.         0.         0.         0.16506238\n",
      "  0.         0.         0.         0.14369148 0.17321804 0.31351713\n",
      "  0.14477796 0.15529335 0.         0.11453577 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.14313473 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13805066 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.22497528 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18196162 0.         0.         0.4277061 ]]\n"
     ]
    }
   ],
   "source": [
    "docIDF = np.log10(len(termMap) / (np.count_nonzero(docVS, axis=0) + 1))\n",
    "docW = docTF * docIDF\n",
    "queW = queTF * docIDF\n",
    "\n",
    "## Step4: caculate similarity\n",
    "from numpy.linalg import norm\n",
    "\n",
    "res = np.zeros((queW.shape[0], docW.shape[0]))\n",
    "for i in range(queW.shape[0]):\n",
    "    for j in range(docW.shape[0]):\n",
    "        res[i, j] = np.dot(queW[i], docW[j]) / (norm(queW[i])*norm(docW[j]))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 35 36 50 43]\n",
      " [28 19 17 15 18]\n",
      " [53  8 17 15 18]\n",
      " [34  2 19 53 18]\n",
      " [ 2 17 16 11 19]\n",
      " [53  7 50 18 23]]\n"
     ]
    }
   ],
   "source": [
    "ind = np.argpartition(res, -5)[:,-5:]\n",
    "resTopK = np.take_along_axis(res, ind, axis=1)\n",
    "resTopKSorted = np.take_along_axis(ind, np.argsort(resTopK), axis=1)\n",
    "print(np.flip(resSortedTopK, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 269.85,
   "position": {
    "height": "291.85px",
    "left": "557px",
    "right": "20px",
    "top": "611px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
