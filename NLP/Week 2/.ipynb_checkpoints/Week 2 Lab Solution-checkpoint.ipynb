{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\n",
      "\n",
      "\n",
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "\n",
      "denni : listen , strang women lie in pond distribut sword is no basi for a system of govern . suprem execut power deriv from a mandat from the mass , not from some farcic aquat ceremoni .\n"
     ]
    }
   ],
   "source": [
    "# porter stemmer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "raw_text = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "print(raw_text)\n",
    "print('\\n')\n",
    "\n",
    "porter_stemmer = nltk.PorterStemmer()\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "stemmed_words = [porter_stemmer.stem(w) for w in tokens]\n",
    "print(stemmed_words)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "stemmed_text = \" \".join(stemmed_words)    \n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\n",
      "\n",
      "\n",
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "\n",
      "denni : listen , strang women lie in pond distribut sword is no basi for a system of govern . suprem execut power deriv from a mandat from the mass , not from some farcic aquat ceremoni .\n"
     ]
    }
   ],
   "source": [
    "# Snowballstemmer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "raw_text2 = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "print(raw_text2)\n",
    "print('\\n')\n",
    "\n",
    "#the stemmer requires a language parameter\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "tokens = nltk.word_tokenize(raw_text1)\n",
    "stemmed_words = [snow_stemmer.stem(w) for w in tokens]\n",
    "print(stemmed_words)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "stemmed_text = \" \".join(stemmed_words)\n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\n",
      "\n",
      "\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n",
      "\n",
      "\n",
      "den : list , strange wom lying in pond distribut sword is no bas for a system of govern . suprem execut pow der from a mand from the mass , not from som farc aqu ceremony .\n"
     ]
    }
   ],
   "source": [
    "# lancasterstemmer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "raw_text1 = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "print(raw_text1)\n",
    "print('\\n')\n",
    "\n",
    "lancaster_stemmer = nltk.LancasterStemmer()\n",
    "tokens = nltk.word_tokenize(raw_text1)\n",
    "stemmed_words = [lancaster_stemmer.stem(w) for w in tokens]\n",
    "print(stemmed_words)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "stemmed_text = \" \".join(stemmed_words)    \n",
    "print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n",
      "\n",
      "\n",
      "DENNIS : Listen , strange woman lying in pond distributing sword is no basis for a system of government . Supreme executive power derives from a mandate from the mass , not from some farcical aquatic ceremony .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('wordnet')\n",
    "\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "print(lemmatized_words)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "lemmatized_text = \" \".join(lemmatized_words)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best', '.']\n",
      "\n",
      "\n",
      "The striped bat are hanging on their foot for best .\n"
     ]
    }
   ],
   "source": [
    "raw_text2 = \"The striped bats are hanging on their feet for best.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(raw_text2)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "print(lemmatized_words)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "lemmatized_text = \" \".join(lemmatized_words)\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval System\n",
    "\n",
    "### Information retrieval system based on ranked retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from os import path\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "\n",
    "porter_stemmer = nltk.PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hiker, demon, creepy, scary, tunnel, stalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Batman, batman beyond, who are you, narrows it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Up, carl, russell, honor, award, scout badge, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tom, jerry, sword, stab, dont care, cartoon, show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Wholesome, comic, dialogue bubble, dog, sleepi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Doug dimmadome, chef hat, long, fast food, res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Empty town, comparison, bustling city, contrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Lord of the rings, lotr, gandalf, pipip, sendi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Geralt, yennefer, pointing, blame, slapstick, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Goofy, college, max ,shock, surprise, reveal, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Gordon Ramsay, pepto bismol, patrick, feeding,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Groot, gunpoint, force, surreal, movie, despic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Having enough, jump, slapstick, fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Cat, possessed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Hotdog, dog, many options</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Jedi, master, lightsaber, block, unexpected, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Joker, you think this is funny, laugh, reactio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Dark, kids, swing, police, dead body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Kylo ren, lightsaber, fight, jesus, comparison...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Lord of the rings, lotr, cast it into fire, de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    docID                                              words\n",
       "0       0         Hiker, demon, creepy, scary, tunnel, stalk\n",
       "1       1  Batman, batman beyond, who are you, narrows it...\n",
       "2       2  Up, carl, russell, honor, award, scout badge, ...\n",
       "3       3  Tom, jerry, sword, stab, dont care, cartoon, show\n",
       "4       4  Wholesome, comic, dialogue bubble, dog, sleepi...\n",
       "5       5  Doug dimmadome, chef hat, long, fast food, res...\n",
       "6       6  Empty town, comparison, bustling city, contrad...\n",
       "7       7  Lord of the rings, lotr, gandalf, pipip, sendi...\n",
       "8       8  Geralt, yennefer, pointing, blame, slapstick, ...\n",
       "9       9  Goofy, college, max ,shock, surprise, reveal, ...\n",
       "10     10  Gordon Ramsay, pepto bismol, patrick, feeding,...\n",
       "11     11  Groot, gunpoint, force, surreal, movie, despic...\n",
       "12     12               Having enough, jump, slapstick, fall\n",
       "13     13                                     Cat, possessed\n",
       "14     14                          Hotdog, dog, many options\n",
       "15     15  Jedi, master, lightsaber, block, unexpected, m...\n",
       "16     16  Joker, you think this is funny, laugh, reactio...\n",
       "17     17               Dark, kids, swing, police, dead body\n",
       "18     18  Kylo ren, lightsaber, fight, jesus, comparison...\n",
       "19     19  Lord of the rings, lotr, cast it into fire, de..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading documents\n",
    "df = pd.read_csv('WordsDataset.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The docID column has been changed to now represent a document by 'D' followed by the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docID</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D0</td>\n",
       "      <td>Hiker, demon, creepy, scary, tunnel, stalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1</td>\n",
       "      <td>Batman, batman beyond, who are you, narrows it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D2</td>\n",
       "      <td>Up, carl, russell, honor, award, scout badge, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D3</td>\n",
       "      <td>Tom, jerry, sword, stab, dont care, cartoon, show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D4</td>\n",
       "      <td>Wholesome, comic, dialogue bubble, dog, sleepi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  docID                                              words\n",
       "0    D0         Hiker, demon, creepy, scary, tunnel, stalk\n",
       "1    D1  Batman, batman beyond, who are you, narrows it...\n",
       "2    D2  Up, carl, russell, honor, award, scout badge, ...\n",
       "3    D3  Tom, jerry, sword, stab, dont care, cartoon, show\n",
       "4    D4  Wholesome, comic, dialogue bubble, dog, sleepi..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.docID = pd.Series([\"D\"+str(ind) for ind in df.docID])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the words column is cleaned by the following steps:\n",
    "- Remove punctuations\n",
    "- Lower case\n",
    "- Strip whitespaces\n",
    "- Remove stopwords\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            hiker  demon  creepy  scary  tunnel  stalk\n",
      "1     batman  batman beyond  who are you  narrows it...\n",
      "2     up  carl  russell  honor  award  scout badge  ...\n",
      "3     tom  jerry  sword  stab  dont care  cartoon  show\n",
      "4     wholesome  comic  dialogue bubble  dog  sleepi...\n",
      "5     doug dimmadome  chef hat  long  fast food  res...\n",
      "6     empty town  comparison  bustling city  contrad...\n",
      "7     lord of the rings  lotr  gandalf  pipip  sendi...\n",
      "8     geralt  yennefer  pointing  blame  slapstick  ...\n",
      "9     goofy  college  max  shock  surprise  reveal  ...\n",
      "10    gordon ramsay  pepto bismol  patrick  feeding ...\n",
      "11    groot  gunpoint  force  surreal  movie  despic...\n",
      "12                 having enough  jump  slapstick  fall\n",
      "13                                       cat  possessed\n",
      "14                            hotdog  dog  many options\n",
      "15    jedi  master  lightsaber  block  unexpected  m...\n",
      "16    joker  you think this is funny  laugh  reactio...\n",
      "17                 dark  kids  swing  police  dead body\n",
      "18    kylo ren  lightsaber  fight  jesus  comparison...\n",
      "19    lord of the rings  lotr  cast it into fire  de...\n",
      "20    master of forbidden knowledge  anime  contradi...\n",
      "21    mike wazowski  face swap  monster inc  stone f...\n",
      "22    mr  moseby  sweet life of zach and cody  stere...\n",
      "23    ninja turtle  growing  old   support  classic ...\n",
      "24    perry  ferb  phineas  looking at both  at the ...\n",
      "25          proud  mabel  mark soos  scaring  slapstick\n",
      "26                        serious guy  ride  unexpected\n",
      "27    meet the pyro  comparison  contradictory  vide...\n",
      "28      stupid intelligence  brain  what is this answer\n",
      "29    terminator  carrying  mr bean  on sofa  holdin...\n",
      "30    the crusade knows no bounds  underwater  diver...\n",
      "31                                  titans  sea  battle\n",
      "32                                     toy  biting  cat\n",
      "33                      two guys  split  slide  combine\n",
      "34    undertaker  randy orton  surprised  reaction  wwe\n",
      "35                        uno  draw 25  option  classic\n",
      "36                        muscle arm  hand shake  alone\n",
      "37    wolverine  cartoon  show third wheel  left out...\n",
      "38        bear  no change  same  fool  cardboard cutout\n",
      "39      keanu reeves  big  small  bench  sad  wholesome\n",
      "40    black bear  white bear  panda  combine  fusion...\n",
      "41                    prefer water  dislike drink  girl\n",
      "42    incredibles  frozone  police  i said freeze  e...\n",
      "43    peter parker  spider man  joker  callmecarson ...\n",
      "44                putin  political  reaction  surprised\n",
      "45                         spongebob  tear  paper truth\n",
      "46                     charlie  phone  reaction  crying\n",
      "47            fallout  puncher  bigger  regret  cartoon\n",
      "48                     vacuum  can t fit  wall  surreal\n",
      "49                          girl  sign  creative  empty\n",
      "50    harry osborn  peter parker  spider man  movie ...\n",
      "51                       jesus  blessing  woman  better\n",
      "52            women smiling  kick  surprised  slapstick\n",
      "53      video game  football  harry kane  umpire  sport\n",
      "Name: words, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df.words = df.words.str.replace(\",\",\" \")\n",
    "df.words = df.words.str.replace(r'\\W',' ')\n",
    "df.words = df.words.str.strip().str.lower()\n",
    "print(df.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary (all uniqye words in the documents) are collected into a set as below. This set of vocabulary is used to match with the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '25', '4', 'alone', 'animated', 'anime', 'announce', 'answer', 'arm', 'award', 'badge', 'batman', 'battle', 'bean', 'bear', 'bench', 'better', 'beyond', 'big', 'bigger', 'bismol', 'biting', 'black', 'blame', 'blessing', 'block', 'body', 'bounds', 'brain', 'broom', 'bubble', 'bustling', 'callmecarson', 'cardboard', 'care', 'carl', 'carrying', 'cartoon', 'cast', 'cat', 'change', 'charlie', 'chef', 'city', 'classic', 'cody', 'college', 'combine', 'comic', 'comparison', 'contradict', 'contradictory', 'creative', 'creepy', 'crossover', 'crusade', 'crying', 'cutout', 'dancing', 'dark', 'dead', 'demon', 'despicable', 'destroy', 'dialogue', 'dimmadome', 'dislike', 'diver', 'dog', 'dont', 'doug', 'dragonballz', 'draw', 'drink', 'employee', 'empty', 'enough', 'excuse', 'face', 'fall', 'fallout', 'fast', 'feeding', 'ferb', 'fight', 'fire', 'fit', 'food', 'fool', 'football', 'forbidden', 'force', 'fortress', 'freeze', 'frozone', 'funny', 'fusion', 'game', 'gandalf', 'geralt', 'girl', 'god', 'goofy', 'gordon', 'groot', 'growing', 'gunpoint', 'guy', 'guys', 'hand', 'harry', 'hat', 'height', 'hidden', 'hiker', 'holding', 'honor', 'hotdog', 'inc', 'incredibles', 'intelligence', 'jedi', 'jerry', 'jesus', 'joker', 'jump', 'kane', 'keanu', 'kick', 'kids', 'knowledge', 'knows', 'kylo', 'laugh', 'left', 'life', 'lightsaber', 'long', 'looking', 'lord', 'lotr', 'mabel', 'man', 'many', 'mark', 'master', 'max', 'meet', 'mike', 'monster', 'moseby', 'movie', 'mr', 'muscle', 'narrows', 'ninja', 'officer', 'old', 'option', 'options', 'orton', 'osborn', 'owner', 'panda', 'paper', 'parker', 'patrick', 'pepto', 'perry', 'peter', 'phineas', 'phone', 'pipip', 'play', 'player', 'pointing', 'police', 'political', 'possessed', 'prefer', 'proud', 'puncher', 'putin', 'pyro', 'ramsay', 'randy', 'reaction', 'record', 'reeves', 'regret', 'ren', 'restaurant', 'reveal', 'ride', 'rings', 'russell', 'sad', 'said', 'scaring', 'scary', 'scout', 'sea', 'sending', 'serious', 'shake', 'shock', 'show', 'sign', 'slapstick', 'sleeping', 'slide', 'small', 'smiling', 'sofa', 'soos', 'spider', 'split', 'spongebob', 'sport', 'stab', 'stalk', 'star', 'starwars', 'step', 'stereotype', 'stone', 'stupid', 'support', 'surprise', 'surprised', 'surreal', 'swap', 'sweet', 'swing', 'sword', 'teaching', 'team', 'tear', 'terminator', 'think', 'third', 'time', 'titans', 'tom', 'town', 'toy', 'truth', 'tunnel', 'turtle', 'two', 'umpire', 'undertaker', 'underwater', 'unexpected', 'uno', 'vacuum', 'video', 'videogame', 'voeyer', 'wall', 'wars', 'water', 'wazowski', 'wheel', 'white', 'wholesome', 'wolverine', 'woman', 'women', 'wwe', 'yennefer', 'zach']\n"
     ]
    }
   ],
   "source": [
    "# print(df.tags.values)\n",
    "\n",
    "all_text = \" \".join(df.words.values)\n",
    "vocab = np.unique(word_tokenize(all_text))\n",
    "vocab = [word for word in vocab if word not in stopwords.words('english')]\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A term-document-matrix is a mapping of every word in the vocabulary to document. Every document is converted to a vector corresponding to frequency of each word appearing in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_document_matrix(data, vocab= None, document_index= 'ID', text= 'text'):\n",
    "    \"\"\"Calculate frequency of term in the document.\n",
    "    \n",
    "    parameter: \n",
    "        data: DataFrame. \n",
    "        Frequency of word calculated against the data.\n",
    "        \n",
    "        vocab: list of strings.\n",
    "        Vocabulary of the documents    \n",
    "        \n",
    "        document_index: str.\n",
    "        Column name for document index in DataFrame passed.\n",
    "        \n",
    "        text: str\n",
    "        Column name containing text for all documents in DataFrame,\n",
    "        \n",
    "    returns:\n",
    "        vocab_index: DataFrame.\n",
    "        DataFrame containing term document matrix.\n",
    "        \"\"\"\n",
    "    \n",
    "    vocab_index = pd.DataFrame(columns=df[document_index], index= vocab).fillna(0)\n",
    "    \n",
    "    for word in vocab_index.index:\n",
    "        \n",
    "        for doc in data[document_index]:\n",
    "            \n",
    "            freq = data[data[document_index] == doc][text].values[0].count(word)\n",
    "            vocab_index.loc[word,doc] = freq\n",
    "    \n",
    "    return vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>docID</th>\n",
       "      <th>D0</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D44</th>\n",
       "      <th>D45</th>\n",
       "      <th>D46</th>\n",
       "      <th>D47</th>\n",
       "      <th>D48</th>\n",
       "      <th>D49</th>\n",
       "      <th>D50</th>\n",
       "      <th>D51</th>\n",
       "      <th>D52</th>\n",
       "      <th>D53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animated</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>women</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wwe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yennefer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zach</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "docID     D0  D1  D2  D3  D4  D5  D6  D7  D8  D9  ...  D44  D45  D46  D47  \\\n",
       "2          0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "25         0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "4          0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "alone      0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "animated   0   1   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "...       ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...  ...  ...   \n",
       "woman      0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "women      0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "wwe        0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "yennefer   0   0   0   0   0   0   0   0   1   0  ...    0    0    0    0   \n",
       "zach       0   0   0   0   0   0   0   0   0   0  ...    0    0    0    0   \n",
       "\n",
       "docID     D48  D49  D50  D51  D52  D53  \n",
       "2           0    0    0    0    0    0  \n",
       "25          0    0    0    0    0    0  \n",
       "4           0    0    0    0    0    0  \n",
       "alone       0    0    0    0    0    0  \n",
       "animated    0    0    0    0    0    0  \n",
       "...       ...  ...  ...  ...  ...  ...  \n",
       "woman       0    0    0    1    0    0  \n",
       "women       0    0    0    0    1    0  \n",
       "wwe         0    0    0    0    0    0  \n",
       "yennefer    0    0    0    0    0    0  \n",
       "zach        0    0    0    0    0    0  \n",
       "\n",
       "[272 rows x 54 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_index = term_document_matrix(df,vocab,'docID','words')\n",
    "similarity_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the term-document-matrix the inverse-document-frequncy is calculated for every document. Using the term-frequencies (tf) and the inverse-document-frequency (idf) the tf-idf score for every word in every document is computed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_score(vocab_index, document_index, inv_df= 'inverse_document_frequency'):\n",
    "    \"\"\"\n",
    "    Calculate tf-idf score for vocabulary in documents\n",
    "    \n",
    "    parameter:\n",
    "        vocab_index: DataFrame.\n",
    "        Term document matrix.\n",
    "        \n",
    "        document_index: list or tuple.\n",
    "        Series containing document ids.\n",
    "        \n",
    "        inv_df: str.\n",
    "        Name of the column with calculated inverse document frequencies.\n",
    "        \n",
    "    returns:\n",
    "        vocab_index: DataFrame.\n",
    "        DataFrame containing term document matrix and document frequencies, inverse document frequencies and tf-idf scores\n",
    "    \"\"\"\n",
    "    total_docx = len(document_index)\n",
    "    vocab_index['document_frequency'] = vocab_index.sum(axis= 1)\n",
    "    vocab_index['inverse_document_frequency'] = np.log2( total_docx / vocab_index['document_frequency'])\n",
    "    \n",
    "    for word in vocab_index.index:\n",
    "        \n",
    "        for doc in document_index:\n",
    "            \n",
    "                tf_idf = np.log2(1 + vocab_index.loc[word,doc]) * np.log2(vocab_index.loc[word][inv_df])\n",
    "                vocab_index.loc[word,'tf_idf_'+doc] = tf_idf\n",
    "    \n",
    "    return vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>docID</th>\n",
       "      <th>D0</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_D44</th>\n",
       "      <th>tf_idf_D45</th>\n",
       "      <th>tf_idf_D46</th>\n",
       "      <th>tf_idf_D47</th>\n",
       "      <th>tf_idf_D48</th>\n",
       "      <th>tf_idf_D49</th>\n",
       "      <th>tf_idf_D50</th>\n",
       "      <th>tf_idf_D51</th>\n",
       "      <th>tf_idf_D52</th>\n",
       "      <th>tf_idf_D53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animated</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.524788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>women</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.524788</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wwe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yennefer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zach</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "docID     D0  D1  D2  D3  D4  D5  D6  D7  D8  D9  ...  tf_idf_D44  tf_idf_D45  \\\n",
       "2          0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "25         0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "4          0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "alone      0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "animated   0   1   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "...       ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...         ...         ...   \n",
       "woman      0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "women      0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "wwe        0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "yennefer   0   0   0   0   0   0   0   0   1   0  ...         0.0         0.0   \n",
       "zach       0   0   0   0   0   0   0   0   0   0  ...         0.0         0.0   \n",
       "\n",
       "docID     tf_idf_D46  tf_idf_D47  tf_idf_D48  tf_idf_D49  tf_idf_D50  \\\n",
       "2                0.0         0.0         0.0         0.0         0.0   \n",
       "25               0.0         0.0         0.0         0.0         0.0   \n",
       "4                0.0         0.0         0.0         0.0         0.0   \n",
       "alone            0.0         0.0         0.0         0.0         0.0   \n",
       "animated         0.0         0.0         0.0         0.0         0.0   \n",
       "...              ...         ...         ...         ...         ...   \n",
       "woman            0.0         0.0         0.0         0.0         0.0   \n",
       "women            0.0         0.0         0.0         0.0         0.0   \n",
       "wwe              0.0         0.0         0.0         0.0         0.0   \n",
       "yennefer         0.0         0.0         0.0         0.0         0.0   \n",
       "zach             0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "docID     tf_idf_D51  tf_idf_D52  tf_idf_D53  \n",
       "2           0.000000    0.000000         0.0  \n",
       "25          0.000000    0.000000         0.0  \n",
       "4           0.000000    0.000000         0.0  \n",
       "alone       0.000000    0.000000         0.0  \n",
       "animated    0.000000    0.000000         0.0  \n",
       "...              ...         ...         ...  \n",
       "woman       2.524788    0.000000         0.0  \n",
       "women       0.000000    2.524788         0.0  \n",
       "wwe         0.000000    0.000000         0.0  \n",
       "yennefer    0.000000    0.000000         0.0  \n",
       "zach        0.000000    0.000000         0.0  \n",
       "\n",
       "[272 rows x 110 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_index = tf_idf_score(similarity_index, df.docID.values)\n",
    "similarity_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation of the above is computation intensive and is only required to do so once unless the database containing documents changes. Thus the above dataframe containing all tf, idfs and the tf-idf scores are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_index.to_csv('term_doc_matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv('term_doc_matrix.csv')\n",
    "test = test.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge assumption which is true is that the user will not input the query in a set format. Hence on the same lines as cleaning the tags, the query is also cleaned :\n",
    "\n",
    "- Remove punctutations\n",
    "- Lower case\n",
    "- Remove whitespaces\n",
    "- Remove stop words\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_processing(query):\n",
    "    \"\"\"\n",
    "    Pre-processing query to accomodate calculations for tf-idf score\n",
    "    \n",
    "    parameter:\n",
    "        query: str.\n",
    "        Textual query input to the system.\n",
    "        \n",
    "    returns:\n",
    "        query: str.\n",
    "        Cleaned string.\n",
    "        \"\"\"\n",
    "    query= re.sub('\\W',' ',query)\n",
    "    query= query.strip().lower()\n",
    "    query= \" \".join([word for word in query.split() if word not in stopwords.words('english')])\n",
    "    \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie kids'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"movie for kids\"\n",
    "query_processing(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every term in the query, if it exists in the vocabulary, then its tf-idf score is calculated and appended to the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_score(vocab_index, query):\n",
    "    \"\"\"\n",
    "    Calculate tf-idf score for query terms\n",
    "    \n",
    "    parameter:\n",
    "        vocab_index: DataFrame.\n",
    "        Term document matrix with inverse document frequency and term frequencies calculated.\n",
    "        \n",
    "        query: str.\n",
    "        Query submitted to the system\n",
    "        \n",
    "    returns:\n",
    "        vocab_index: DataFrame.\n",
    "        Term document matrix with tf-idf scores for terms per document and query terms.\n",
    "    \"\"\"\n",
    "    for word in np.unique(query.split()):\n",
    "        \n",
    "        freq = query.count(word)\n",
    "        \n",
    "        if word in vocab_index.index:\n",
    "            \n",
    "            tf_idf = np.log2(1+freq) * np.log2(vocab_index.loc[word].inverse_document_frequency)\n",
    "            vocab_index.loc[word,\"query_tf_idf\"] = tf_idf\n",
    "            vocab_index['query_tf_idf'].fillna(0, inplace=True)\n",
    "    \n",
    "    return vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D0</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>tf_idf_D45</th>\n",
       "      <th>tf_idf_D46</th>\n",
       "      <th>tf_idf_D47</th>\n",
       "      <th>tf_idf_D48</th>\n",
       "      <th>tf_idf_D49</th>\n",
       "      <th>tf_idf_D50</th>\n",
       "      <th>tf_idf_D51</th>\n",
       "      <th>tf_idf_D52</th>\n",
       "      <th>tf_idf_D53</th>\n",
       "      <th>query_tf_idf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>animated</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.524788</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>women</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.524788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wwe</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yennefer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zach</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            D0  D1  D2  D3  D4  D5  D6  D7  D8  D9  ...  tf_idf_D45  \\\n",
       "Unnamed: 0                                          ...               \n",
       "2            0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "25           0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "4            0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "alone        0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "animated     0   1   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "...         ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ...         ...   \n",
       "woman        0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "women        0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "wwe          0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "yennefer     0   0   0   0   0   0   0   0   1   0  ...         0.0   \n",
       "zach         0   0   0   0   0   0   0   0   0   0  ...         0.0   \n",
       "\n",
       "            tf_idf_D46  tf_idf_D47  tf_idf_D48  tf_idf_D49  tf_idf_D50  \\\n",
       "Unnamed: 0                                                               \n",
       "2                  0.0         0.0         0.0         0.0         0.0   \n",
       "25                 0.0         0.0         0.0         0.0         0.0   \n",
       "4                  0.0         0.0         0.0         0.0         0.0   \n",
       "alone              0.0         0.0         0.0         0.0         0.0   \n",
       "animated           0.0         0.0         0.0         0.0         0.0   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "woman              0.0         0.0         0.0         0.0         0.0   \n",
       "women              0.0         0.0         0.0         0.0         0.0   \n",
       "wwe                0.0         0.0         0.0         0.0         0.0   \n",
       "yennefer           0.0         0.0         0.0         0.0         0.0   \n",
       "zach               0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "            tf_idf_D51  tf_idf_D52  tf_idf_D53  query_tf_idf  \n",
       "Unnamed: 0                                                    \n",
       "2             0.000000    0.000000         0.0           0.0  \n",
       "25            0.000000    0.000000         0.0           0.0  \n",
       "4             0.000000    0.000000         0.0           0.0  \n",
       "alone         0.000000    0.000000         0.0           0.0  \n",
       "animated      0.000000    0.000000         0.0           0.0  \n",
       "...                ...         ...         ...           ...  \n",
       "woman         2.524788    0.000000         0.0           0.0  \n",
       "women         0.000000    2.524788         0.0           0.0  \n",
       "wwe           0.000000    0.000000         0.0           0.0  \n",
       "yennefer      0.000000    0.000000         0.0           0.0  \n",
       "zach          0.000000    0.000000         0.0           0.0  \n",
       "\n",
       "[272 rows x 111 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query= \"movie for kids\"\n",
    "similarity_index = query_score(test,query)\n",
    "similarity_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to find the cosine similarities between the query and documents. The cosine similarity determines how similar the two vectors are - Document vector and the query vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vocab_index, document_index, query_scores):\n",
    "    \"\"\"\n",
    "    Calculates cosine similarity between the documents and query\n",
    "    \n",
    "    parameter:\n",
    "        \n",
    "        vocab_index: DataFrame.\n",
    "        DataFrame containing tf-idf score per term for every document and for the query terms.\n",
    "        \n",
    "        document_index: list.\n",
    "        List of document ids.\n",
    "        \n",
    "        query_scores: str.\n",
    "        Column name in DataFrame containing query term tf-idf scores.\n",
    "        \n",
    "    returns:\n",
    "        cosine_scores: Series.\n",
    "        Cosine similarity scores of every document.\n",
    "    \"\"\"\n",
    "    cosine_scores = {}\n",
    "    \n",
    "    query_scalar = np.sqrt(sum(vocab_index[query_scores] ** 2))\n",
    "    \n",
    "    for doc in document_index:\n",
    "        \n",
    "        doc_scalar = np.sqrt(sum(vocab_index[doc] ** 2))\n",
    "        dot_prod = sum(vocab_index[doc] * vocab_index[query_scores])\n",
    "        cosine = (dot_prod / (query_scalar * doc_scalar))\n",
    "        \n",
    "        cosine_scores[doc] = cosine\n",
    "        \n",
    "    return pd.Series(cosine_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D0     0.000000\n",
       "D1     0.000000\n",
       "D2     0.411276\n",
       "D3     0.000000\n",
       "D4     0.000000\n",
       "D5     0.000000\n",
       "D6     0.000000\n",
       "D7     0.175137\n",
       "D8     0.000000\n",
       "D9     0.000000\n",
       "D10    0.000000\n",
       "D11    0.202231\n",
       "D12    0.000000\n",
       "D13    0.000000\n",
       "D14    0.000000\n",
       "D15    0.175137\n",
       "D16    0.202231\n",
       "D17    0.354640\n",
       "D18    0.156647\n",
       "D19    0.187230\n",
       "D20    0.000000\n",
       "D21    0.142999\n",
       "D22    0.000000\n",
       "D23    0.000000\n",
       "D24    0.000000\n",
       "D25    0.000000\n",
       "D26    0.000000\n",
       "D27    0.000000\n",
       "D28    0.000000\n",
       "D29    0.000000\n",
       "D30    0.000000\n",
       "D31    0.000000\n",
       "D32    0.000000\n",
       "D33    0.000000\n",
       "D34    0.000000\n",
       "D35    0.000000\n",
       "D36    0.000000\n",
       "D37    0.000000\n",
       "D38    0.000000\n",
       "D39    0.000000\n",
       "D40    0.000000\n",
       "D41    0.000000\n",
       "D42    0.165121\n",
       "D43    0.000000\n",
       "D44    0.000000\n",
       "D45    0.000000\n",
       "D46    0.000000\n",
       "D47    0.000000\n",
       "D48    0.000000\n",
       "D49    0.000000\n",
       "D50    0.165121\n",
       "D51    0.000000\n",
       "D52    0.000000\n",
       "D53    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosines = cosine_similarity(similarity_index, df.docID.values, 'query_tf_idf')\n",
    "cosines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the cosine score for every document with the query is calculated. The documents are ranked with respect to their score. The top 'k' documents, here 10, are retrieved in the form of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_index(data,cosine_scores, document_index):\n",
    "    \"\"\"\n",
    "    Retrieves indices for the corresponding document cosine scores\n",
    "    \n",
    "    parameters:\n",
    "        data: DataFrame.\n",
    "        DataFrame containing document ids and text.\n",
    "        \n",
    "        cosine_scores: Series.\n",
    "        Series containing document cosine scores.\n",
    "        \n",
    "        document_index: str.\n",
    "        Column name containing document ids in data.\n",
    "        \n",
    "    returns:\n",
    "        data: DataFrame.\n",
    "        Original DataFrame with cosine scores added as column.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = data.set_index(document_index)\n",
    "    data['scores'] = cosine_scores\n",
    "    \n",
    "    return data.reset_index().sort_values('scores',ascending=False).head(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2, 17, 16, 11, 19, 15, 7, 50, 42, 18], dtype='int64')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = retrieve_index(df, cosines, 'docID')\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indices are the top 10 most relevant according to the query.  \n",
    "The function below, summarizes, by calling, all the above written individual functions. Only the below functions needs to be called to run the system and retrieve indices for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_system(query):\n",
    "    \"\"\"\n",
    "    Perform a retrieval from the indexes based on the query \n",
    "    and return the document ids that are similar to the query\n",
    "    \n",
    "    paramters:\n",
    "        query: str.\n",
    "        Query submitted to the system.\n",
    "        \n",
    "    returns:\n",
    "        indices: list.\n",
    "        List of document indices which are most relevant to the query.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv('TagsDatabase.csv')\n",
    "\n",
    " \n",
    "    df.docID = pd.Series([\"D\"+str(ind) for ind in df.docID])\n",
    "\n",
    "    df.words = df.words.str.replace(\",\",\" \")\n",
    "    df.words = df.words.str.replace(r'\\W',' ')\n",
    "    df.words = df.words.str.strip().str.lower()\n",
    "    \n",
    "    if not path.exists('term_doc_matrix.csv'):    \n",
    "\n",
    "        all_text = \" \".join(df.words.values)\n",
    "        vocab = np.unique(word_tokenize(all_text))\n",
    "        vocab = [word for word in vocab if word not in stopwords.words('english')]\n",
    "\n",
    "        similarity_index = term_document_matrix(df,vocab,'docID','words')\n",
    "        similarity_index = tf_idf_score(similarity_index, df.docID.values)\n",
    "        \n",
    "    else:\n",
    "        similarity_index = pd.read_csv('term_doc_matrix.csv')\n",
    "        similarity_index = similarity_index.set_index('Unnamed: 0')\n",
    "        \n",
    "    query = query_processing(query)\n",
    "    similarity_index = query_score(similarity_index,query)\n",
    "    \n",
    "    cosines = cosine_similarity(similarity_index, df.docID.values, 'query_tf_idf')\n",
    "    indices = retrieve_index(df, cosines, 'docID')\n",
    "    \n",
    "    return list(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 17, 16, 11, 19, 7, 15, 42, 50, 18]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_system('movie for kids')\n",
    "\n",
    "#information_system('lack of intelligence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
