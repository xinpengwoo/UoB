{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDEbF2x0UnuW"
   },
   "source": [
    "# Text-Classification with Naive Bayes\n",
    "This week you have learned about text classfication using ML algorithm Naive Bayes. In the lab this week, you will learn how to go through the steps of training a text-classifier using some of the techniques talked about in the lecture this week.\n",
    "\n",
    "We will use [NLTK](https://www.nltk.org/) and [Scikit-learn](https://scikit-learn.org/stable/) as we go through the lab sheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ppq4e_tZLsj"
   },
   "source": [
    "## Import your data\n",
    "\n",
    "The dataset you will be using can be downloaded from this [link](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/). Download the zip file and make sure you read through the readme file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "GgckxzLncWsN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Tag                                                SMS\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "...    ...                                                ...\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568   ham               Will ü b going to esplanade fr home?\n",
      "5569   ham  Pity, * was in mood for that. So...any other s...\n",
      "5570   ham  The guy did some bitching but I acted like i'd...\n",
      "5571   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5572 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the text file into a DataFrame\n",
    "data = pd.read_csv('/home/xinpeng/UoB/NLP/Week5/smsspamcollection/SMSSpamCollection', header=None, sep='\\t')\n",
    "data.columns = ['Tag', 'SMS']\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Md8gfnZkly-",
    "outputId": "5512a14c-cf57-4c68-ebd7-53303fd17b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 5572 examples.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have a total of {len(data)} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbqbXgoalR1v"
   },
   "source": [
    "Now change the label column in the dataframe so that we have:\n",
    "* 0 for ham\n",
    "* 1 for spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Fau8q3Tm1rl"
   },
   "source": [
    "We want to also explore if we have a class imbalance and what our data looks like. Make sure you understand what you are classifying before diving in. \n",
    "\n",
    "Use the power of pandas dataframe to count the number of examples for each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tag'] = data['Tag'].replace(['ham', 'spam'], ['0', '1'])\n",
    "data['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN5RKadTZPXM"
   },
   "source": [
    "## Preprocess your data\n",
    "Now that we have seen what our data looks like, we can begin to think about how we might pre-process our data. We can do a simple pre-processing and then, if needed, we can do more, such as lemmatise etc.\n",
    "\n",
    "1. words are lower case\n",
    "2. tokenize\n",
    "3. stop-word removal\n",
    "4. punctuation and non-alpha character removal\n",
    "5. Lemmatise the words in the text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "3QfdMjj1Gt8i"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "lPFva10jAYVE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/xinpeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/xinpeng/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "tZtUu6QfGoyB"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9getKXqnJc5H"
   },
   "source": [
    "We make everything lower case and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD_uw9wlL3aW"
   },
   "source": [
    "Remove stopwords, non alphabetic characters and punctuation, and lemmatise the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VAwt_wCDFCU"
   },
   "source": [
    "Compare the original text to the preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tag                                                SMS  \\\n",
      "0      0  Go until jurong point, crazy.. Available only ...   \n",
      "1      0                      Ok lar... Joking wif u oni...   \n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3      0  U dun say so early hor... U c already then say...   \n",
      "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
      "...   ..                                                ...   \n",
      "5567   1  This is the 2nd time we have tried 2 contact u...   \n",
      "5568   0               Will ü b going to esplanade fr home?   \n",
      "5569   0  Pity, * was in mood for that. So...any other s...   \n",
      "5570   0  The guy did some bitching but I acted like i'd...   \n",
      "5571   0                         Rofl. Its true to its name   \n",
      "\n",
      "                                          tokenized_SMS  \\\n",
      "0     [go, until, jurong, point, ,, crazy, .., avail...   \n",
      "1              [ok, lar, ..., joking, wif, u, oni, ...]   \n",
      "2     [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
      "3     [u, dun, say, so, early, hor, ..., u, c, alrea...   \n",
      "4     [nah, i, do, n't, think, he, goes, to, usf, ,,...   \n",
      "...                                                 ...   \n",
      "5567  [this, is, the, 2nd, time, we, have, tried, 2,...   \n",
      "5568    [will, ü, b, going, to, esplanade, fr, home, ?]   \n",
      "5569  [pity, ,, *, was, in, mood, for, that, ., so, ...   \n",
      "5570  [the, guy, did, some, bitching, but, i, acted,...   \n",
      "5571                [rofl, ., its, true, to, its, name]   \n",
      "\n",
      "                                                removal  \\\n",
      "0     [go, until, jurong, point, crazy, available, o...   \n",
      "1                        [ok, lar, joking, wif, u, oni]   \n",
      "2     [free, entry, in, a, wkly, comp, to, win, fa, ...   \n",
      "3     [u, dun, say, so, early, hor, u, c, already, t...   \n",
      "4     [nah, i, don, t, think, he, goes, to, usf, he,...   \n",
      "...                                                 ...   \n",
      "5567  [this, is, the, nd, time, we, have, tried, con...   \n",
      "5568          [will, b, going, to, esplanade, fr, home]   \n",
      "5569  [pity, was, in, mood, for, that, so, any, othe...   \n",
      "5570  [the, guy, did, some, bitching, but, i, acted,...   \n",
      "5571                   [rofl, its, true, to, its, name]   \n",
      "\n",
      "                                               removal2  \n",
      "0     [go, jurong, point, crazy, available, bugis, n...  \n",
      "1                        [ok, lar, joking, wif, u, oni]  \n",
      "2     [free, entry, wkly, comp, win, fa, cup, final,...  \n",
      "3         [u, dun, say, early, hor, u, c, already, say]  \n",
      "4        [nah, think, goes, usf, lives, around, though]  \n",
      "...                                                 ...  \n",
      "5567  [nd, time, tried, contact, u, u, pound, prize,...  \n",
      "5568                    [b, going, esplanade, fr, home]  \n",
      "5569                          [pity, mood, suggestions]  \n",
      "5570  [guy, bitching, acted, like, interested, buyin...  \n",
      "5571                                 [rofl, true, name]  \n",
      "\n",
      "[5572 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#data['tokenized_SMS'] = data.apply(lambda row: nltk.word_tokenize(row['SMS'].lower()), axis=1)\n",
    "#data['removal'] = data.apply(lambda row: re.findall(r'[a-zA-Z]+', ' '.join(row['tokenized_SMS'])), axis=1)\n",
    "data['removal'] = data.apply(lambda row: re.findall(r'[a-zA-Z]+', row['SMS'].lower()), axis=1)\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "data['removal2'] = data.apply(lambda row: [word for word in row['removal'] if not word in nltk_stopwords], axis=1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5PU18VEZTNN"
   },
   "source": [
    "## Split the data\n",
    "Now we will split our data using sklearn into our train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "pFM_3jVtCWIs"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6_tsX2YZe-8"
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "Let's experiment with feature selection. We will test two different methods for this:\n",
    "\n",
    "\n",
    "1.   Word frequency\n",
    "2.   Mutual Information - Tfidf\n",
    "\n",
    "Implement both. We will train and test our algorithms with both to see which one works better for our data.\n",
    "\n",
    "Hint: this is where we are vectorizing our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pM56c7VVKGZS"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUbpJBcnL94I"
   },
   "source": [
    "Take a look to see what our feature names are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JK0_aQ0YKAZM"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6iGUv3MaIB2"
   },
   "source": [
    "## Train the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9tEQTVYPUkP"
   },
   "source": [
    "Train the model using the data vectorized using the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5BJG2BIPdFR"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxvNrkMlPbmt"
   },
   "source": [
    "Train the model using the data vectorized using the tfidf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBlvT1vkbMBy"
   },
   "source": [
    "## Test and Results\n",
    "Now that we have a trained classifier, let's test to see if it generalises well to unseen examples. Let's compare the F-1 score for the two feature selection methods we used earlier.\n",
    "\n",
    "Make sure to note which one seems to work beter for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTeFw3ekUinU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4WJNn0WS2pE"
   },
   "source": [
    "Test the count model on the test data. Then print the confusion matrix and classification report comparing the gold labels to predictions from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA1aAkRMSpWE"
   },
   "source": [
    "Test the tfidf model on the test data. Then print the confusion matrix and classification report comparing the gold labels to predictions from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T3MWW6hccif"
   },
   "source": [
    "# Bonus: Multiclass classification\n",
    "\n",
    "Now that you have implemented a binary classifier, let's try to do the same but this time for multiple classes. We will be working with the ______ dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37GgM-QJTVR-"
   },
   "source": [
    "Use the [20 newsgroups dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) available on sklearn dataset library.\n",
    "\n",
    "Implement naive bayes but for 5 classes (out of the 20 avaialable) on this dataset. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "247.85px",
    "left": "838px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
