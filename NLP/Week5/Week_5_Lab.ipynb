{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDEbF2x0UnuW"
   },
   "source": [
    "# Text-Classification with Naive Bayes\n",
    "This week you have learned about text classfication using ML algorithm Naive Bayes. In the lab this week, you will learn how to go through the steps of training a text-classifier using some of the techniques talked about in the lecture this week.\n",
    "\n",
    "We will use [NLTK](https://www.nltk.org/) and [Scikit-learn](https://scikit-learn.org/stable/) as we go through the lab sheet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Ppq4e_tZLsj"
   },
   "source": [
    "## Import your data\n",
    "\n",
    "The dataset you will be using can be downloaded from this [link](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/). Download the zip file and make sure you read through the readme file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "GgckxzLncWsN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tag                                                SMS\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Read the text file into a DataFrame\n",
    "data = pd.read_csv('/home/xinpeng/UoB/NLP/Week5/smsspamcollection/SMSSpamCollection', header=None, sep='\\t')\n",
    "data.columns = ['Tag', 'SMS']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Md8gfnZkly-",
    "outputId": "5512a14c-cf57-4c68-ebd7-53303fd17b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 5572 examples.\n"
     ]
    }
   ],
   "source": [
    "print(f'We have a total of {len(data)} examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbqbXgoalR1v"
   },
   "source": [
    "Now change the label column in the dataframe so that we have:\n",
    "* 0 for ham\n",
    "* 1 for spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Fau8q3Tm1rl"
   },
   "source": [
    "We want to also explore if we have a class imbalance and what our data looks like. Make sure you understand what you are classifying before diving in. \n",
    "\n",
    "Use the power of pandas dataframe to count the number of examples for each of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: Tag, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tag'] = data['Tag'].replace(['ham', 'spam'], ['0', '1'])\n",
    "data['Tag'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN5RKadTZPXM"
   },
   "source": [
    "## Preprocess your data\n",
    "Now that we have seen what our data looks like, we can begin to think about how we might pre-process our data. We can do a simple pre-processing and then, if needed, we can do more, such as lemmatise etc.\n",
    "\n",
    "1. words are lower case\n",
    "2. tokenize\n",
    "3. stop-word removal\n",
    "4. punctuation and non-alpha character removal\n",
    "5. Lemmatise the words in the text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "3QfdMjj1Gt8i"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "lPFva10jAYVE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/xinpeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/xinpeng/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "tZtUu6QfGoyB"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9getKXqnJc5H"
   },
   "source": [
    "We make everything lower case and tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD_uw9wlL3aW"
   },
   "source": [
    "Remove stopwords, non alphabetic characters and punctuation, and lemmatise the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VAwt_wCDFCU"
   },
   "source": [
    "Compare the original text to the preprocessed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>SMS</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>nd time tried contact u u pound prize claim ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "      <td>b going esplanade fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>0</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>pity mood suggestion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>0</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitching acted like interested buying some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>0</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tag                                                SMS  \\\n",
       "0      0  Go until jurong point, crazy.. Available only ...   \n",
       "1      0                      Ok lar... Joking wif u oni...   \n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      0  U dun say so early hor... U c already then say...   \n",
       "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...   ..                                                ...   \n",
       "5567   1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   0               Will ü b going to esplanade fr home?   \n",
       "5569   0  Pity, * was in mood for that. So...any other s...   \n",
       "5570   0  The guy did some bitching but I acted like i'd...   \n",
       "5571   0                         Rofl. Its true to its name   \n",
       "\n",
       "                                           preprocessed  \n",
       "0     go jurong point crazy available bugis n great ...  \n",
       "1                               ok lar joking wif u oni  \n",
       "2     free entry wkly comp win fa cup final tkts st ...  \n",
       "3                   u dun say early hor u c already say  \n",
       "4                   nah think go usf life around though  \n",
       "...                                                 ...  \n",
       "5567  nd time tried contact u u pound prize claim ea...  \n",
       "5568                          b going esplanade fr home  \n",
       "5569                               pity mood suggestion  \n",
       "5570  guy bitching acted like interested buying some...  \n",
       "5571                                     rofl true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk import stem\n",
    "def preprocess(data):\n",
    "    #data['tokenized_SMS'] = data.apply(lambda row: nltk.word_tokenize(row['SMS'].lower()), axis=1)\n",
    "    #data['removal'] = data.apply(lambda row: re.findall(r'[a-zA-Z]+', ' '.join(row['tokenized_SMS'])), axis=1)\n",
    "    #tokenize and reserve only alphabetic characters\n",
    "    data['preprocessed'] = data.apply(lambda row: re.findall(r'[a-zA-Z]+', row['SMS'].lower()), axis=1)\n",
    "    #remove stopwords\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "    data['preprocessed'] = data.apply(lambda row: [word for word in row['preprocessed'] if not word in nltk_stopwords], axis=1)\n",
    "    #lemmatization\n",
    "    wnl = stem.WordNetLemmatizer()\n",
    "    data['preprocessed'] = data.apply(lambda row: [wnl.lemmatize(word) for word in row['preprocessed']], axis=1)\n",
    "    #convert list to string\n",
    "    data['preprocessed'] = data.apply(lambda row: \" \".join(row['preprocessed']), axis=1)\n",
    "    return data\n",
    "\n",
    "data = preprocess(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5PU18VEZTNN"
   },
   "source": [
    "## Split the data\n",
    "Now we will split our data using sklearn into our train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "pFM_3jVtCWIs"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test = train_test_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6_tsX2YZe-8"
   },
   "source": [
    "## Feature selection\n",
    "\n",
    "Let's experiment with feature selection. We will test two different methods for this:\n",
    "\n",
    "\n",
    "1.   Word frequency\n",
    "2.   Mutual Information - Tfidf\n",
    "\n",
    "Implement both. We will train and test our algorithms with both to see which one works better for our data.\n",
    "\n",
    "Hint: this is where we are vectorizing our data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "pM56c7VVKGZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x7098 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 45138 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_WordFrequency = vectorizer.fit_transform(data['preprocessed'])\n",
    "X_WordFrequency\n",
    "# we now use the same vectorizer used and fit with our train data with test set\n",
    "# X_test_counts = vectorizer.transform(X_test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUbpJBcnL94I"
   },
   "source": [
    "Take a look to see what our feature names are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aah',\n",
       " 'aaniye',\n",
       " 'aaooooright',\n",
       " 'aathi',\n",
       " 'ab',\n",
       " 'abbey',\n",
       " 'abdomen',\n",
       " 'abeg',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'abi',\n",
       " 'ability',\n",
       " 'abiola',\n",
       " 'abj',\n",
       " 'able',\n",
       " 'abnormally',\n",
       " 'aboutas',\n",
       " 'abroad',\n",
       " 'absence',\n",
       " 'absolutely',\n",
       " 'absolutly',\n",
       " 'abstract',\n",
       " 'abt',\n",
       " 'abta',\n",
       " 'aburo',\n",
       " 'abuse',\n",
       " 'abuser',\n",
       " 'ac',\n",
       " 'academic',\n",
       " 'acc',\n",
       " 'accent',\n",
       " 'accenture',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accessible',\n",
       " 'accidant',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accommodation',\n",
       " 'accommodationvouchers',\n",
       " 'accomodate',\n",
       " 'accomodations',\n",
       " 'accordin',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accounting',\n",
       " 'accumulation',\n",
       " 'achan',\n",
       " 'ache',\n",
       " 'achieve',\n",
       " 'acid',\n",
       " 'acknowledgement',\n",
       " 'acl',\n",
       " 'acnt',\n",
       " 'aco',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'activate',\n",
       " 'active',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'addamsfa',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addie',\n",
       " 'adding',\n",
       " 'address',\n",
       " 'adewale',\n",
       " 'adi',\n",
       " 'adjustable',\n",
       " 'admin',\n",
       " 'administrator',\n",
       " 'admirer',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'adore',\n",
       " 'adoring',\n",
       " 'adp',\n",
       " 'adress',\n",
       " 'adrian',\n",
       " 'adrink',\n",
       " 'adsense',\n",
       " 'adult',\n",
       " 'advance',\n",
       " 'adventure',\n",
       " 'adventuring',\n",
       " 'advice',\n",
       " 'advise',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'ae',\n",
       " 'aeronautics',\n",
       " 'aeroplane',\n",
       " 'afew',\n",
       " 'affair',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affidavit',\n",
       " 'afford',\n",
       " 'afghanistan',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'aft',\n",
       " 'afternon',\n",
       " 'afternoon',\n",
       " 'afterwards',\n",
       " 'aftr',\n",
       " 'ag',\n",
       " 'agalla',\n",
       " 'age',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agidhane',\n",
       " 'aging',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahead',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ahmad',\n",
       " 'ahold',\n",
       " 'aid',\n",
       " 'aig',\n",
       " 'aight',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'airtel',\n",
       " 'aiya',\n",
       " 'aiyah',\n",
       " 'aiyar',\n",
       " 'aiyo',\n",
       " 'aj',\n",
       " 'ajith',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akon',\n",
       " 'al',\n",
       " 'alaikkum',\n",
       " 'alaipayuthe',\n",
       " 'albi',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'aldrine',\n",
       " 'alert',\n",
       " 'alertfrom',\n",
       " 'aletter',\n",
       " 'alex',\n",
       " 'alfie',\n",
       " 'algarve',\n",
       " 'algebra',\n",
       " 'algorithm',\n",
       " 'ali',\n",
       " 'alian',\n",
       " 'alibi',\n",
       " 'alive',\n",
       " 'allah',\n",
       " 'allalo',\n",
       " 'allday',\n",
       " 'alle',\n",
       " 'allo',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'alrite',\n",
       " 'also',\n",
       " 'alter',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'alto',\n",
       " 'aluable',\n",
       " 'alwa',\n",
       " 'always',\n",
       " 'alwys',\n",
       " 'am',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'ambitious',\n",
       " 'ambrith',\n",
       " 'american',\n",
       " 'ami',\n",
       " 'amigo',\n",
       " 'amk',\n",
       " 'amla',\n",
       " 'amma',\n",
       " 'ammae',\n",
       " 'ammo',\n",
       " 'amnow',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amore',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'amplikater',\n",
       " 'amrca',\n",
       " 'amrita',\n",
       " 'amt',\n",
       " 'amused',\n",
       " 'amy',\n",
       " 'an',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'analysis',\n",
       " 'anand',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andres',\n",
       " 'andrew',\n",
       " 'andros',\n",
       " 'angel',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'animation',\n",
       " 'anjie',\n",
       " 'anjola',\n",
       " 'anna',\n",
       " 'annie',\n",
       " 'anniversary',\n",
       " 'annoncement',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'annoyin',\n",
       " 'annoying',\n",
       " 'anonymous',\n",
       " 'anot',\n",
       " 'another',\n",
       " 'ansr',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answerin',\n",
       " 'answering',\n",
       " 'answr',\n",
       " 'antelope',\n",
       " 'antha',\n",
       " 'anthony',\n",
       " 'anti',\n",
       " 'antibiotic',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anyones',\n",
       " 'anyplaces',\n",
       " 'anythiing',\n",
       " 'anythin',\n",
       " 'anything',\n",
       " 'anythingtomorrow',\n",
       " 'anytime',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'aom',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'ape',\n",
       " 'apeshit',\n",
       " 'aphex',\n",
       " 'apnt',\n",
       " 'apo',\n",
       " 'apologetic',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apology',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'appeal',\n",
       " 'appear',\n",
       " 'appendix',\n",
       " 'applausestore',\n",
       " 'apple',\n",
       " 'applebees',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'applyed',\n",
       " 'applying',\n",
       " 'appointment',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'approve',\n",
       " 'approved',\n",
       " 'approx',\n",
       " 'apps',\n",
       " 'appt',\n",
       " 'appy',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'aproach',\n",
       " 'apt',\n",
       " 'aptitude',\n",
       " 'aq',\n",
       " 'aquarius',\n",
       " 'ar',\n",
       " 'arab',\n",
       " 'arabian',\n",
       " 'arcade',\n",
       " 'archive',\n",
       " 'ard',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'arestaurant',\n",
       " 'aretaking',\n",
       " 'areyouunique',\n",
       " 'argentina',\n",
       " 'argh',\n",
       " 'argue',\n",
       " 'arguing',\n",
       " 'argument',\n",
       " 'aries',\n",
       " 'arise',\n",
       " 'arises',\n",
       " 'arithmetic',\n",
       " 'arm',\n",
       " 'armand',\n",
       " 'armenia',\n",
       " 'arng',\n",
       " 'arngd',\n",
       " 'arnt',\n",
       " 'around',\n",
       " 'aroundn',\n",
       " 'arr',\n",
       " 'arrange',\n",
       " 'arranging',\n",
       " 'arrested',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrow',\n",
       " 'arsenal',\n",
       " 'art',\n",
       " 'artist',\n",
       " 'arty',\n",
       " 'arul',\n",
       " 'arun',\n",
       " 'as',\n",
       " 'asa',\n",
       " 'asap',\n",
       " 'asda',\n",
       " 'ash',\n",
       " 'ashley',\n",
       " 'ashwini',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'asjesus',\n",
       " 'ask',\n",
       " 'askd',\n",
       " 'asked',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'aslamalaikkum',\n",
       " 'asleep',\n",
       " 'asp',\n",
       " 'aspect',\n",
       " 'assessment',\n",
       " 'asshole',\n",
       " 'assistance',\n",
       " 'associate',\n",
       " 'asssssholeeee',\n",
       " 'assume',\n",
       " 'assumed',\n",
       " 'asthere',\n",
       " 'asthma',\n",
       " 'astne',\n",
       " 'astoundingly',\n",
       " 'astrology',\n",
       " 'astronomer',\n",
       " 'asus',\n",
       " 'asusual',\n",
       " 'ate',\n",
       " 'athletic',\n",
       " 'athome',\n",
       " 'atlanta',\n",
       " 'atlast',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'atrocious',\n",
       " 'attach',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attempt',\n",
       " 'atten',\n",
       " 'attend',\n",
       " 'attended',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'attracts',\n",
       " 'attributed',\n",
       " 'atural',\n",
       " 'auction',\n",
       " 'audiitions',\n",
       " 'audition',\n",
       " 'audrey',\n",
       " 'audrie',\n",
       " 'august',\n",
       " 'aunt',\n",
       " 'auntie',\n",
       " 'aunty',\n",
       " 'aust',\n",
       " 'australia',\n",
       " 'authorise',\n",
       " 'auto',\n",
       " 'autocorrect',\n",
       " 'av',\n",
       " 'ava',\n",
       " 'availa',\n",
       " 'available',\n",
       " 'avalarr',\n",
       " 'avatar',\n",
       " 'avble',\n",
       " 'ave',\n",
       " 'avenge',\n",
       " 'avent',\n",
       " 'avenue',\n",
       " 'avin',\n",
       " 'avo',\n",
       " 'avoid',\n",
       " 'avoiding',\n",
       " 'avoids',\n",
       " 'await',\n",
       " 'awaiting',\n",
       " 'awake',\n",
       " 'award',\n",
       " 'awarded',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awkward',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'ax',\n",
       " 'axel',\n",
       " 'axis',\n",
       " 'ay',\n",
       " 'ayn',\n",
       " 'ayo',\n",
       " 'ba',\n",
       " 'baaaaaaaabe',\n",
       " 'baaaaabe',\n",
       " 'babe',\n",
       " 'baby',\n",
       " 'babygoodbye',\n",
       " 'babyjontet',\n",
       " 'babysit',\n",
       " 'babysitting',\n",
       " 'bac',\n",
       " 'back',\n",
       " 'backdoor',\n",
       " 'backwards',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badly',\n",
       " 'badrith',\n",
       " 'bag',\n",
       " 'bahamas',\n",
       " 'baig',\n",
       " 'bailiff',\n",
       " 'bajarangabali',\n",
       " 'bak',\n",
       " 'bakra',\n",
       " 'bakrid',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'baller',\n",
       " 'balloon',\n",
       " 'bam',\n",
       " 'bambling',\n",
       " 'ban',\n",
       " 'band',\n",
       " 'bandage',\n",
       " 'bang',\n",
       " 'bangb',\n",
       " 'bangbabes',\n",
       " 'bank',\n",
       " 'banned',\n",
       " 'banneduk',\n",
       " 'banter',\n",
       " 'bao',\n",
       " 'bar',\n",
       " 'barbie',\n",
       " 'barcelona',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'bari',\n",
       " 'barkley',\n",
       " 'barmed',\n",
       " 'barolla',\n",
       " 'barred',\n",
       " 'barrel',\n",
       " 'barring',\n",
       " 'barry',\n",
       " 'base',\n",
       " 'based',\n",
       " 'bash',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basket',\n",
       " 'basketball',\n",
       " 'basq',\n",
       " 'bat',\n",
       " 'batch',\n",
       " 'batchlor',\n",
       " 'bath',\n",
       " 'bathe',\n",
       " 'bathing',\n",
       " 'bathroom',\n",
       " 'batsman',\n",
       " 'batt',\n",
       " 'battery',\n",
       " 'battle',\n",
       " 'bawling',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbd',\n",
       " 'bbdeluxe',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bcaz',\n",
       " 'bck',\n",
       " 'bcm',\n",
       " 'bcmsfwc',\n",
       " 'bcoz',\n",
       " 'bcs',\n",
       " 'bcum',\n",
       " 'bcums',\n",
       " 'bcz',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'bead',\n",
       " 'bear',\n",
       " 'beating',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'bec',\n",
       " 'becaus',\n",
       " 'becausethey',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoz',\n",
       " 'becz',\n",
       " 'bed',\n",
       " 'bedbut',\n",
       " 'bedreal',\n",
       " 'bedrm',\n",
       " 'bedroom',\n",
       " 'beeen',\n",
       " 'beehoon',\n",
       " 'beendropping',\n",
       " 'beer',\n",
       " 'beerage',\n",
       " 'befor',\n",
       " 'beforehand',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'begging',\n",
       " 'begin',\n",
       " 'begun',\n",
       " 'behalf',\n",
       " 'behave',\n",
       " 'behind',\n",
       " 'bein',\n",
       " 'believe',\n",
       " 'belive',\n",
       " 'bell',\n",
       " 'bellearlier',\n",
       " 'belligerent',\n",
       " 'belly',\n",
       " 'belong',\n",
       " 'belongs',\n",
       " 'belovd',\n",
       " 'beloved',\n",
       " 'belt',\n",
       " 'ben',\n",
       " 'bend',\n",
       " 'beneath',\n",
       " 'beneficiary',\n",
       " 'benefit',\n",
       " 'benny',\n",
       " 'bergkamp',\n",
       " 'beside',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'beth',\n",
       " 'betta',\n",
       " 'better',\n",
       " 'bettersn',\n",
       " 'bettr',\n",
       " 'beverage',\n",
       " 'bevy',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bf',\n",
       " 'bffs',\n",
       " 'bfore',\n",
       " 'bhaji',\n",
       " 'bhamb',\n",
       " 'bhaskar',\n",
       " 'bhayandar',\n",
       " 'bian',\n",
       " 'biatch',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billed',\n",
       " 'billing',\n",
       " 'billion',\n",
       " 'billy',\n",
       " 'bilo',\n",
       " 'bimbo',\n",
       " 'bin',\n",
       " 'biola',\n",
       " 'bird',\n",
       " 'birla',\n",
       " 'biro',\n",
       " 'birth',\n",
       " 'birthdate',\n",
       " 'birthday',\n",
       " 'bishan',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bitching',\n",
       " 'bite',\n",
       " 'biz',\n",
       " 'bk',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blacko',\n",
       " 'blah',\n",
       " 'blake',\n",
       " 'blame',\n",
       " 'blank',\n",
       " 'blanked',\n",
       " 'blanket',\n",
       " 'blastin',\n",
       " 'bleak',\n",
       " 'bleh',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blessing',\n",
       " 'blimey',\n",
       " 'blind',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blog',\n",
       " 'blogging',\n",
       " 'blogspot',\n",
       " 'bloke',\n",
       " 'blonde',\n",
       " 'bloo',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'bloomberg',\n",
       " 'blow',\n",
       " 'blowing',\n",
       " 'blown',\n",
       " 'blu',\n",
       " 'blue',\n",
       " 'bluetooth',\n",
       " 'bluetoothhdset',\n",
       " 'bluff',\n",
       " 'blur',\n",
       " 'bluray',\n",
       " 'bmw',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'boatin',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'boggy',\n",
       " 'bognor',\n",
       " 'bold',\n",
       " 'bollox',\n",
       " 'boltblue',\n",
       " 'bomb',\n",
       " 'bone',\n",
       " 'bong',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'boob',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'bookedthe',\n",
       " 'booking',\n",
       " 'bookmark',\n",
       " 'bookshelf',\n",
       " 'boooo',\n",
       " 'boost',\n",
       " 'booty',\n",
       " 'bootydelious',\n",
       " 'borderline',\n",
       " 'bored',\n",
       " 'borin',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'bos',\n",
       " 'boston',\n",
       " 'bot',\n",
       " 'bother',\n",
       " 'bothering',\n",
       " 'bottle',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'boundary',\n",
       " 'bout',\n",
       " 'bowa',\n",
       " 'bowl',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boye',\n",
       " 'boyf',\n",
       " 'boyfriend',\n",
       " 'boytoy',\n",
       " 'bp',\n",
       " 'bpo',\n",
       " 'bra',\n",
       " 'brah',\n",
       " 'brain',\n",
       " 'braindance',\n",
       " 'brainless',\n",
       " 'brainy',\n",
       " 'brand',\n",
       " 'brandy',\n",
       " 'brat',\n",
       " 'braved',\n",
       " 'bray',\n",
       " 'brb',\n",
       " 'brdget',\n",
       " 'bread',\n",
       " 'breadstick',\n",
       " 'break',\n",
       " 'breaker',\n",
       " 'breakfast',\n",
       " 'breakin',\n",
       " 'breaking',\n",
       " 'breath',\n",
       " 'breathe',\n",
       " 'breather',\n",
       " 'breathing',\n",
       " 'breeze',\n",
       " 'breezy',\n",
       " 'brekkie',\n",
       " 'bremoved',\n",
       " 'bribe',\n",
       " 'bridal',\n",
       " 'bridge',\n",
       " 'bridgwater',\n",
       " 'brief',\n",
       " 'bright',\n",
       " 'brighten',\n",
       " 'brilliant',\n",
       " 'brilliantly',\n",
       " 'brin',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'brings',\n",
       " 'brisk',\n",
       " 'brison',\n",
       " 'bristol',\n",
       " 'british',\n",
       " 'britney',\n",
       " 'bro',\n",
       " 'broad',\n",
       " 'broadband',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brolly',\n",
       " 'bros',\n",
       " 'broth',\n",
       " 'brothas',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'brownie',\n",
       " 'browse',\n",
       " 'browser',\n",
       " 'browsin',\n",
       " 'bruce',\n",
       " 'brum',\n",
       " 'bruv',\n",
       " 'bslvyl',\n",
       " 'bsn',\n",
       " 'bsnl',\n",
       " 'bstfrnd',\n",
       " 'bt',\n",
       " 'bthere',\n",
       " 'btw',\n",
       " 'btwn',\n",
       " 'bubbletext',\n",
       " 'buck',\n",
       " 'bud',\n",
       " 'buddy',\n",
       " 'budget',\n",
       " 'buen',\n",
       " 'buff',\n",
       " 'buffet',\n",
       " 'buffy',\n",
       " 'bugis',\n",
       " 'build',\n",
       " 'building',\n",
       " 'built',\n",
       " 'bulb',\n",
       " 'bull',\n",
       " 'bullshit',\n",
       " 'bun',\n",
       " 'bunch',\n",
       " 'bundle',\n",
       " 'bunker',\n",
       " 'burden',\n",
       " 'burger',\n",
       " 'burgundy',\n",
       " 'burial',\n",
       " 'burn',\n",
       " 'burning',\n",
       " 'burnt',\n",
       " 'burrito',\n",
       " 'bus',\n",
       " 'busetop',\n",
       " 'business',\n",
       " 'busty',\n",
       " 'busy',\n",
       " 'butt',\n",
       " 'buttheres',\n",
       " 'butting',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buying',\n",
       " 'buz',\n",
       " 'buzy',\n",
       " 'buzz',\n",
       " 'buzzzz',\n",
       " 'bw',\n",
       " 'bx',\n",
       " 'byatch',\n",
       " 'bye',\n",
       " 'cab',\n",
       " 'cabin',\n",
       " 'cable',\n",
       " 'cafe',\n",
       " 'cage',\n",
       " 'cake',\n",
       " 'cal',\n",
       " 'calculated',\n",
       " 'calculation',\n",
       " 'cali',\n",
       " 'calicut',\n",
       " 'california',\n",
       " 'call',\n",
       " 'callback',\n",
       " 'callcost',\n",
       " 'calld',\n",
       " 'called',\n",
       " 'caller',\n",
       " 'callertune',\n",
       " 'callfreefone',\n",
       " 'callin',\n",
       " 'calling',\n",
       " 'callon',\n",
       " 'calm',\n",
       " 'cam',\n",
       " 'camcorder',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'campus',\n",
       " 'camry',\n",
       " 'canada',\n",
       " 'canal',\n",
       " 'canary',\n",
       " 'cancel',\n",
       " 'canceled',\n",
       " 'cancelled',\n",
       " 'cancer',\n",
       " 'canlove',\n",
       " 'cann',\n",
       " 'canname',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'cantdo',\n",
       " 'canteen',\n",
       " 'cap',\n",
       " 'capacity',\n",
       " 'capital',\n",
       " 'cappuccino',\n",
       " 'captain',\n",
       " 'captaining',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cardiff',\n",
       " 'cardin',\n",
       " 'care',\n",
       " 'careabout',\n",
       " 'cared',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'careless',\n",
       " 'caring',\n",
       " 'carlie',\n",
       " 'carlin',\n",
       " 'carlos',\n",
       " 'carly',\n",
       " 'carolina',\n",
       " 'caroline',\n",
       " 'carpark',\n",
       " 'carry',\n",
       " 'carryin',\n",
       " 'carton',\n",
       " 'cartoon',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cashbin',\n",
       " 'cashed',\n",
       " 'cashto',\n",
       " 'casing',\n",
       " 'cast',\n",
       " 'casting',\n",
       " 'castor',\n",
       " 'casualty',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'category',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'causing',\n",
       " 'cave',\n",
       " 'caveboy',\n",
       " 'cbe',\n",
       " 'cc',\n",
       " 'ccna',\n",
       " 'cd',\n",
       " 'cdgt',\n",
       " 'cedar',\n",
       " 'ceiling',\n",
       " 'celeb',\n",
       " 'celebrate',\n",
       " 'celebrated',\n",
       " 'celebration',\n",
       " 'cell',\n",
       " 'census',\n",
       " 'center',\n",
       " 'centre',\n",
       " 'century',\n",
       " 'cer',\n",
       " 'cereal',\n",
       " 'ceri',\n",
       " 'certainly',\n",
       " 'certificate',\n",
       " 'cfca',\n",
       " 'ch',\n",
       " 'cha',\n",
       " 'chachi',\n",
       " 'chad',\n",
       " 'chain',\n",
       " 'challenge',\n",
       " 'challenging',\n",
       " 'champ',\n",
       " 'champlaxigating',\n",
       " 'champneys',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'channel',\n",
       " 'chap',\n",
       " 'chapel',\n",
       " 'chapter',\n",
       " 'character',\n",
       " 'charge',\n",
       " ...]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "JK0_aQ0YKAZM"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def getFeatureVector2(data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_Tfidf = vectorizer.fit_transform(data)\n",
    "    return X_Tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6iGUv3MaIB2"
   },
   "source": [
    "## Train the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9tEQTVYPUkP"
   },
   "source": [
    "Train the model using the data vectorized using the count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "N5BJG2BIPdFR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_WordFrequency, data['Tag'])\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxvNrkMlPbmt"
   },
   "source": [
    "Train the model using the data vectorized using the tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf2 = MultinomialNB()\n",
    "# clf2.fit(getFeatureVector2(data_train['preprocessed']), data_train['Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBlvT1vkbMBy"
   },
   "source": [
    "## Test and Results\n",
    "Now that we have a trained classifier, let's test to see if it generalises well to unseen examples. Let's compare the F-1 score for the two feature selection methods we used earlier.\n",
    "\n",
    "Make sure to note which one seems to work beter for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "jTeFw3ekUinU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4WJNn0WS2pE"
   },
   "source": [
    "Test the count model on the test data. Then print the confusion matrix and classification report comparing the gold labels to predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.86      0.87      0.86      1202\n",
      "        spam       0.13      0.13      0.13       191\n",
      "\n",
      "    accuracy                           0.77      1393\n",
      "   macro avg       0.50      0.50      0.50      1393\n",
      "weighted avg       0.76      0.77      0.76      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#?????\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion_matrix(y_true, y_pred)\n",
    "target_names = ['ham', 'spam']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA1aAkRMSpWE"
   },
   "source": [
    "Test the tfidf model on the test data. Then print the confusion matrix and classification report comparing the gold labels to predictions from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T3MWW6hccif"
   },
   "source": [
    "# Bonus: Multiclass classification\n",
    "\n",
    "Now that you have implemented a binary classifier, let's try to do the same but this time for multiple classes. We will be working with the ______ dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37GgM-QJTVR-"
   },
   "source": [
    "Use the [20 newsgroups dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) available on sklearn dataset library.\n",
    "\n",
    "Implement naive bayes but for 5 classes (out of the 20 avaialable) on this dataset. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "247.85px",
    "left": "838px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
