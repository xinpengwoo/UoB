{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization: Task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  4\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization using regular expression implementation\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "\n",
    "sentences = re.split(r\"(?<=[.!?])\\s\", sample_text)\n",
    "print(\"Number of sentences: \", len(sentences))\n",
    "print(np.array(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 4\n",
      "Sample text sentences :-\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization using NLTK's implementation\n",
    "\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "default_st = nltk.sent_tokenize\n",
    "\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. \" \n",
    "               \"The US has unveiled the world's most powerful supercomputer called 'Summit', \" \n",
    "               \"beating the previous record-holder China's Sunway TaihuLight. With a peak performance \"\n",
    "               \"of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, \"\n",
    "               \"which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, \"\n",
    "               \"which reportedly take up the size of two tennis courts.\")\n",
    "\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences :-')\n",
    "print(np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\", \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\", 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.', 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text = \"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\"\n",
    "\n",
    "output = sent_tokenize(text)\n",
    "print(len(output))\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization: Task-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Prof', 'Mark', 'Lee', 'Ph.D', 'at', 'the', 'University', 'of', 'Birmingham', 'U.K', 'says', 'Crungus', 'is', 'merely', 'a', 'composite', 'of', 'data', 'that', 'Craiyon', 'has', 'seen', 'I', 'think', 'we', 'could', 'say', 'that', \"it's\", 'producing', 'things', 'which', 'are', 'original', 'he', 'says', 'But', 'they', 'are', 'based', 'on', 'previous', 'examples', 'It', 'could', 'be', 'just', 'a', 'blended', 'image', \"that's\", 'come', 'from', 'multiple', 'sources', 'And', 'it', 'looks', 'very', 'scary', 'right']\n",
      "\n",
      "\n",
      "Prof Mark Lee Ph.D at the University of Birmingham U.K says Crungus is merely a composite of data that Craiyon has seen I think we could say that it's producing things which are original he says But they are based on previous examples It could be just a blended image that's come from multiple sources And it looks very scary right\n"
     ]
    }
   ],
   "source": [
    "########################     Tokenization using regular expression    ##########################\n",
    "\n",
    "import re\n",
    "\n",
    "raw_text = \"\"\"Prof. Mark Lee Ph.D at the University of Birmingham, U.K, says Crungus is merely a composite of data that Craiyon has seen. \"I think we could say that it's producing things which are original,\" he says. \"But they are based on previous examples. It could be just a blended image that's come from multiple sources. And it looks very scary, right?\"\"\"\n",
    "\n",
    "tokenized_word = re.findall(r\"\\w+(?:[-.']\\w+)*|'\\S\\w*\", raw_text)\n",
    "print(tokenized_word)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "tokenized_text = \" \".join(tokenized_word)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prof Mark Lee Ph D at the University of Birmingham U K says Crungus is merely a composite of data that Craiyon has seen I think we could say that it s producing things which are original he says But they are based on previous examples It could be just a blended image that s come from multiple sources And it looks very scary right\n",
      "\n",
      "\n",
      "['Prof', 'Mark', 'Lee', 'Ph', 'D', 'at', 'the', 'University', 'of', 'Birmingham', 'U', 'K', 'says', 'Crungus', 'is', 'merely', 'a', 'composite', 'of', 'data', 'that', 'Craiyon', 'has', 'seen', 'I', 'think', 'we', 'could', 'say', 'that', 'it', 's', 'producing', 'things', 'which', 'are', 'original', 'he', 'says', 'But', 'they', 'are', 'based', 'on', 'previous', 'examples', 'It', 'could', 'be', 'just', 'a', 'blended', 'image', 'that', 's', 'come', 'from', 'multiple', 'sources', 'And', 'it', 'looks', 'very', 'scary', 'right']\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's RegexpTokenizer implementation   ###################################\n",
    "\n",
    "\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(raw_text)\n",
    "tokenized_text = \" \".join(tokenized_text)\n",
    "print(tokenized_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "tokenized_text = tokenized_text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Prof.',\n",
       " 'Mark',\n",
       " 'Lee',\n",
       " 'Ph.D',\n",
       " 'at',\n",
       " 'the',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Birmingham',\n",
       " ',',\n",
       " 'U.K',\n",
       " ',',\n",
       " 'says',\n",
       " 'Crungus',\n",
       " 'is',\n",
       " 'merely',\n",
       " 'a',\n",
       " 'composite',\n",
       " 'of',\n",
       " 'data',\n",
       " 'that',\n",
       " 'Craiyon',\n",
       " 'has',\n",
       " 'seen',\n",
       " '.',\n",
       " '``',\n",
       " 'I',\n",
       " 'think',\n",
       " 'we',\n",
       " 'could',\n",
       " 'say',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'producing',\n",
       " 'things',\n",
       " 'which',\n",
       " 'are',\n",
       " 'original',\n",
       " ',',\n",
       " \"''\",\n",
       " 'he',\n",
       " 'says',\n",
       " '.',\n",
       " '``',\n",
       " 'But',\n",
       " 'they',\n",
       " 'are',\n",
       " 'based',\n",
       " 'on',\n",
       " 'previous',\n",
       " 'examples',\n",
       " '.',\n",
       " 'It',\n",
       " 'could',\n",
       " 'be',\n",
       " 'just',\n",
       " 'a',\n",
       " 'blended',\n",
       " 'image',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'come',\n",
       " 'from',\n",
       " 'multiple',\n",
       " 'sources',\n",
       " '.',\n",
       " 'And',\n",
       " 'it',\n",
       " 'looks',\n",
       " 'very',\n",
       " 'scary',\n",
       " ',',\n",
       " 'right',\n",
       " '?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's word_tokenize implementation   ###################################\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prof. Mark Lee Ph.D at the University of Birmingham, U.K, says Crungus is merely a composite of data that Craiyon has seen. \"I think we could say that it's producing things which are original,\" he says. \"But they are based on previous examples. It could be just a blended image that's come from multiple sources. And it looks very scary, right\n",
      "\n",
      "\n",
      "['Prof.', 'Mark', 'Lee', 'Ph.D', 'at', 'the', 'University', 'of', 'Birmingham,', 'U.K,', 'says', 'Crungus', 'is', 'merely', 'a', 'composite', 'of', 'data', 'that', 'Craiyon', 'has', 'seen.', '\"I', 'think', 'we', 'could', 'say', 'that', \"it's\", 'producing', 'things', 'which', 'are', 'original,\"', 'he', 'says.', '\"But', 'they', 'are', 'based', 'on', 'previous', 'examples.', 'It', 'could', 'be', 'just', 'a', 'blended', 'image', \"that's\", 'come', 'from', 'multiple', 'sources.', 'And', 'it', 'looks', 'very', 'scary,', 'right']\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using Python string module   ###################################\n",
    "\n",
    "\n",
    "from string import punctuation as punc\n",
    "\n",
    "for words in raw_text:\n",
    "    if words in punc:\n",
    "        tokenized_word = raw_text.replace(words, '')\n",
    "print(tokenized_word)\n",
    "\n",
    "print('\\n')\n",
    "tokenized_word = tokenized_word.split()\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prof Mark Lee PhD at the University of Birmingham UK says Crungus is merely a composite of data that Craiyon has seen I think we could say that its producing things which are original he says But they are based on previous examples It could be just a blended image thats come from multiple sources And it looks very scary right\n",
      "\n",
      "\n",
      "['Prof', 'Mark', 'Lee', 'PhD', 'at', 'the', 'University', 'of', 'Birmingham', 'UK', 'says', 'Crungus', 'is', 'merely', 'a', 'composite', 'of', 'data', 'that', 'Craiyon', 'has', 'seen', 'I', 'think', 'we', 'could', 'say', 'that', 'its', 'producing', 'things', 'which', 'are', 'original', 'he', 'says', 'But', 'they', 'are', 'based', 'on', 'previous', 'examples', 'It', 'could', 'be', 'just', 'a', 'blended', 'image', 'thats', 'come', 'from', 'multiple', 'sources', 'And', 'it', 'looks', 'very', 'scary', 'right']\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using Python string module with translate method  ###################################\n",
    "\n",
    "import string\n",
    "\n",
    "tokenized_text = raw_text.translate(str.maketrans('','', string.punctuation))\n",
    "print(tokenized_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "tokenized_text = tokenized_text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization: Task-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'at', 'all', 'Soup', 'does', 'very', 'well', 'without-Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered']\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using regular expression  ###################################\n",
    "\n",
    "import re\n",
    "\n",
    "raw_text2 = \"\"\"I won't have any pepper in my kitchen at all. Soup does very well without-Maybe it's always pepper that makes people hot-tempered.\"\"\"\n",
    "\n",
    "\n",
    "tokenized_word = re.findall(r\"\\w+(?:[-.']\\w+)*|'\\S\\w*\", raw_text2)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'at', 'all', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's RegexpTokenizer implementation  ###################################\n",
    "\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(raw_text2)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without-Maybe',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot-tempered',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's word_tokenize implementation  ###################################\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(raw_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization: Task-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'had', 'a', 'meeting', 'with', 'our', 'industrial', 'partner', 'on', 'October', '22nd', '2022', 'it', 'was', 'scheduled', 'for', '2', 'hours', 'but', 'it', 'ended', 'up', 'lasting', 'for', '2.5', 'hours', 'The', 'meeting', 'was', 'productive', 'and', 'we', 'were', 'able', 'to', 'finalize', 'the', 'budget', 'for', 'next', 'quarter', 'which', 'is', '$250,000']\n",
      "\n",
      "\n",
      "We had a meeting with our industrial partner on October 22nd 2022 it was scheduled for 2 hours but it ended up lasting for 2.5 hours The meeting was productive and we were able to finalize the budget for next quarter which is $250,000\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using regular expression  ###################################\n",
    "\n",
    "raw_text3 = \"\"\"We had a meeting with our industrial partner on October 22nd, 2022, it was scheduled for 2 hours, but it ended up lasting for 2.5 hours. The meeting was productive and we were able to finalize the budget for next quarter, which is $250,000.\"\"\"\n",
    "\n",
    "tokenized_word = re.findall(r\"(\\w+(?:[-.']\\w+)*|[$]\\d+[,]?\\d+|'\\S\\w*)\", raw_text3)\n",
    "print(tokenized_word)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "tokenized_text = \" \".join(tokenized_word)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'had', 'a', 'meeting', 'with', 'our', 'industrial', 'partner', 'on', 'October', '22nd', '2022', 'it', 'was', 'scheduled', 'for', '2', 'hours', 'but', 'it', 'ended', 'up', 'lasting', 'for', '2', '5', 'hours', 'The', 'meeting', 'was', 'productive', 'and', 'we', 'were', 'able', 'to', 'finalize', 'the', 'budget', 'for', 'next', 'quarter', 'which', 'is', '250', '000']\n",
      "\n",
      "\n",
      "We had a meeting with our industrial partner on October 22nd 2022 it was scheduled for 2 hours but it ended up lasting for 2.5 hours The meeting was productive and we were able to finalize the budget for next quarter which is $250,000\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's RegexpTokenizer implementation  ###################################\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(raw_text3)\n",
    "print(tokenized_text)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "tokenized_text = \" \".join(tokenized_word)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'had',\n",
       " 'a',\n",
       " 'meeting',\n",
       " 'with',\n",
       " 'our',\n",
       " 'industrial',\n",
       " 'partner',\n",
       " 'on',\n",
       " 'October',\n",
       " '22nd',\n",
       " ',',\n",
       " '2022',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'scheduled',\n",
       " 'for',\n",
       " '2',\n",
       " 'hours',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'lasting',\n",
       " 'for',\n",
       " '2.5',\n",
       " 'hours',\n",
       " '.',\n",
       " 'The',\n",
       " 'meeting',\n",
       " 'was',\n",
       " 'productive',\n",
       " 'and',\n",
       " 'we',\n",
       " 'were',\n",
       " 'able',\n",
       " 'to',\n",
       " 'finalize',\n",
       " 'the',\n",
       " 'budget',\n",
       " 'for',\n",
       " 'next',\n",
       " 'quarter',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " '$',\n",
       " '250,000',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's word_tokenizer implementation  ###################################\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(raw_text3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization: Task-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had a meeting with industrial partner and Ph.D researchers on October 22nd, 2022, it was scheduled for 2 hours, but it ended up lasting for 2.5 hours. The meeting was productive and we were able to finalize the budget for next quarter, which is $250,000. I won't have any pepper in my kitchen at all. Soup does very well without-Maybe it's always pepper that makes people hot-tempered.\n",
      "\n",
      "\n",
      "\n",
      "['We', 'had', 'a', 'meeting', 'with', 'industrial', 'partner', 'and', 'Ph.D', 'researchers', 'on', 'October', '22nd', '2022', 'it', 'was', 'scheduled', 'for', '2', 'hours', 'but', 'it', 'ended', 'up', 'lasting', 'for', '2.5', 'hours', 'The', 'meeting', 'was', 'productive', 'and', 'we', 'were', 'able', 'to', 'finalize', 'the', 'budget', 'for', 'next', 'quarter', 'which', 'is', '$250,000', 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'at', 'all', 'Soup', 'does', 'very', 'well', 'without-Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered']\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using regular expression  ###################################\n",
    "\n",
    "raw_text4 = \"\"\"We had a meeting with industrial partner and Ph.D researchers on October 22nd, 2022, it was scheduled for 2 hours, but it ended up lasting for 2.5 hours. The meeting was productive and we were able to finalize the budget for next quarter, which is $250,000. I won't have any pepper in my kitchen at all. Soup does very well without-Maybe it's always pepper that makes people hot-tempered.\"\"\"\n",
    "print(raw_text4)\n",
    "\n",
    "print('\\n\\n')\n",
    "tokenized_word = re.findall(r\"(\\w+(?:[-.']\\w+)*|[$]\\d+[,]?\\d+|'\\S\\w*)\", raw_text4)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'had', 'a', 'meeting', 'with', 'industrial', 'partner', 'and', 'Ph', 'D', 'researchers', 'on', 'October', '22nd', '2022', 'it', 'was', 'scheduled', 'for', '2', 'hours', 'but', 'it', 'ended', 'up', 'lasting', 'for', '2', '5', 'hours', 'The', 'meeting', 'was', 'productive', 'and', 'we', 'were', 'able', 'to', 'finalize', 'the', 'budget', 'for', 'next', 'quarter', 'which', 'is', '250', '000', 'I', 'won', 't', 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'at', 'all', 'Soup', 'does', 'very', 'well', 'without', 'Maybe', 'it', 's', 'always', 'pepper', 'that', 'makes', 'people', 'hot', 'tempered']\n"
     ]
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's RegexpTokenizer implementation  ###################################\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenized_text = tokenizer.tokenize(raw_text4)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'had',\n",
       " 'a',\n",
       " 'meeting',\n",
       " 'with',\n",
       " 'industrial',\n",
       " 'partner',\n",
       " 'and',\n",
       " 'Ph.D',\n",
       " 'researchers',\n",
       " 'on',\n",
       " 'October',\n",
       " '22nd',\n",
       " ',',\n",
       " '2022',\n",
       " ',',\n",
       " 'it',\n",
       " 'was',\n",
       " 'scheduled',\n",
       " 'for',\n",
       " '2',\n",
       " 'hours',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'lasting',\n",
       " 'for',\n",
       " '2.5',\n",
       " 'hours',\n",
       " '.',\n",
       " 'The',\n",
       " 'meeting',\n",
       " 'was',\n",
       " 'productive',\n",
       " 'and',\n",
       " 'we',\n",
       " 'were',\n",
       " 'able',\n",
       " 'to',\n",
       " 'finalize',\n",
       " 'the',\n",
       " 'budget',\n",
       " 'for',\n",
       " 'next',\n",
       " 'quarter',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " '$',\n",
       " '250,000',\n",
       " '.',\n",
       " 'I',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'at',\n",
       " 'all',\n",
       " '.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without-Maybe',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot-tempered',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################  Tokenization using NLTK's word_tokenize implementation  ###################################\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(raw_text4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_stopwords = \"\"\"The life-span of trees is determined by growth rings. These can be seen if the tree is cut down or in cores taken from the edge to the center of the tree. Correct determination is only possible for trees which make growth rings, generally those which occur in seasonal climates. Trees in uniform non-seasonal tropical climates are always growing and do not have distinct growth rings. It is also only possible for trees which are solid to the center of the tree; many very old trees become hollow as the dead heartwood decays away. For some of these species, age estimates have been made on the basis of extrapolating current growth rates, but the results are usually little better than guesses or speculation. White proposed a method of estimating the age of large and veteran trees in the United Kingdom by correlation between a trees stem diameter, growth character and age.\"\"\"\n",
    "print(text_with_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_with_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12548/1385613337.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# convert text to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtext_with_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_with_stopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# convert text to tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_with_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# let'f first print the available list of stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# printing the ntlk stop words list\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "print(nltk_stopwords)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# convert text to lower case\n",
    "text_with_stopwords = text_with_stopwords.lower()\n",
    "\n",
    "# convert text to tokens\n",
    "text_tokenized = word_tokenize(text_with_stopwords)\n",
    "print(text_tokenized)\n",
    "\n",
    "# remove stop words\n",
    "text_without_sw = [word for word in text_tokenized if not word in stopwords.words()]\n",
    "\n",
    "print(text_without_sw)\n",
    "print('\\n\\n')\n",
    "\n",
    "# if you want to rejoin the tokens to make sentences\n",
    "filtered_sentence = (\" \").join(text_without_sw)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_with_stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12548/251974015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtext_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_with_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtext_without_sw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_tokenized\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_with_stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "# Add new stop words to default stop words list, and remove the stop words by using updated stop words list\n",
    "\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "# adding new word to existing nltk stop word list\n",
    "nltk_stopwords = nltk_stopwords.append('made')\n",
    "\n",
    "\n",
    "text_tokenized = word_tokenize(text_with_stopwords)\n",
    "text_without_sw = [word for word in text_tokenized if not word in stopwords.words()]\n",
    "\n",
    "print(text_without_sw)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "filtered_sentence = (\" \").join(text_without_sw)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file for writing and create it if it does not exist\n",
    "f = open(\"text_without_sw.txt\", \"w\")\n",
    "\n",
    "# write to the file\n",
    "f.write(str(text_without_sw))\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
