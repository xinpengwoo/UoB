{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Natural Language Processing</center></h1>\n",
    "<h3><center>Lab-2</center></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization and Stop Words\n",
    "\n",
    "\n",
    "In this lab session, we will focus on text tokenization and stop word removal, which are very important preprocessing steps for text analysis. We will perform these preprocessing steps by using the [NLTK](https://www.nltk.org/api/nltk.html) library and also by writing simple regular expressions.\n",
    "\n",
    "\n",
    "\n",
    "### Sentence Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-1: Use the following text for sentence tokenization:  \n",
    "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\"\n",
    "  \n",
    "Implement a tokenizer by using a regular expression to do the following tasks:  \n",
    "- Tokenize the sample text into sentences.\n",
    "- Count the sentences in the sample text.\n",
    "- Print the identified sentences.\n",
    "\n",
    "Perform sentence tokenization by using [NLTK sentence tokenizer](https://www.nltk.org/api/nltk.tokenize.html) and compare the results with the results produced with regular expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"US unveils world's most powerful supercomputer, beats China.\", \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\", 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.', 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\", \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\", 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.', 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "s = '''US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.'''\n",
    "reSent = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s'\n",
    "sentByRe = re.split(reSent, s)\n",
    "\n",
    "print(sentByRe)\n",
    "print(sent_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-1: Use the following text for word tokenization:  \n",
    "Prof. Mark Lee Ph.D at the University of Birmingham, U.K, says Crungus is merely a composite of data that Craiyon has seen. \"I think we could say that it\\'s producing things which are original,\" he says. \"But they are based on previous examples. It could be just a blended image that\\'s come from multiple sources. And it looks very scary, right?\"\n",
    "\n",
    "- Implement a word tokenizer using the [Python string module](https://docs.python.org/3/library/string.html) to tokenize the above given text.\n",
    "- Implement a word tokenizer to tokenize the given text using a [regular expression](https://docs.python.org/3/library/re.html).\n",
    "- Perform word tokenization by using [NLTK word tokenizer](https://www.nltk.org/api/nltk.tokenize.html) and compare the results with the results generated with regular expression.\n",
    "\n",
    "Make sure all the words containing apostrophes remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Prof.',\n",
       "  'Mark',\n",
       "  'Lee',\n",
       "  'Ph.D',\n",
       "  'at',\n",
       "  'the',\n",
       "  'University',\n",
       "  'of',\n",
       "  'Birmingham',\n",
       "  ',',\n",
       "  'U.K',\n",
       "  ',',\n",
       "  'says',\n",
       "  'Crungus',\n",
       "  'is',\n",
       "  'merely',\n",
       "  'a',\n",
       "  'composite',\n",
       "  'of',\n",
       "  'data',\n",
       "  'that',\n",
       "  'Craiyon',\n",
       "  'has',\n",
       "  'seen',\n",
       "  '.'],\n",
       " ['``',\n",
       "  'I',\n",
       "  'think',\n",
       "  'we',\n",
       "  'could',\n",
       "  'say',\n",
       "  'that',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'producing',\n",
       "  'things',\n",
       "  'which',\n",
       "  'are',\n",
       "  'original',\n",
       "  ',',\n",
       "  \"''\",\n",
       "  'he',\n",
       "  'says',\n",
       "  '.'],\n",
       " ['``', 'But', 'they', 'are', 'based', 'on', 'previous', 'examples', '.'],\n",
       " ['It',\n",
       "  'could',\n",
       "  'be',\n",
       "  'just',\n",
       "  'a',\n",
       "  'blended',\n",
       "  'image',\n",
       "  'that',\n",
       "  \"'s\",\n",
       "  'come',\n",
       "  'from',\n",
       "  'multiple',\n",
       "  'sources',\n",
       "  '.'],\n",
       " ['And', 'it', 'looks', 'very', 'scary', ',', 'right', '?', \"''\"]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#String split\n",
    "delimiters = \"?\", \"\\\"\", \". \"\n",
    "s2 = '''Prof. Mark Lee Ph.D at the University of Birmingham, U.K, says Crungus is merely a composite of data that Craiyon has seen. \"I think we could say that it's producing things which are original,\" he says. \"But they are based on previous examples. It could be just a blended image that's come from multiple sources. And it looks very scary, right?\"'''\n",
    "#s2.split(\" \")\n",
    "#NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "[word_tokenize(t) for t in sent_tokenize(s2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-2: Use the following text for word tokenization: \n",
    "\n",
    "I won't have any pepper in my kitchen at all. Soup does very well without-Maybe it's always pepper that makes people hot-tempered.\n",
    "\n",
    "- Implement a word tokenizer to tokenize the given text using a [regular expression](https://docs.python.org/3/library/re.html).\n",
    "- Perform word tokenization by using [NLTK word tokenizer](https://www.nltk.org/api/nltk.tokenize.html) and compare the results with the results generated with regular expression.\n",
    "\n",
    "Make sure all the words containing apostrophes and multi-words remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I',\n",
       "  'wo',\n",
       "  \"n't\",\n",
       "  'have',\n",
       "  'any',\n",
       "  'pepper',\n",
       "  'in',\n",
       "  'my',\n",
       "  'kitchen',\n",
       "  'at',\n",
       "  'all',\n",
       "  '.'],\n",
       " ['Soup',\n",
       "  'does',\n",
       "  'very',\n",
       "  'well',\n",
       "  'without-Maybe',\n",
       "  'it',\n",
       "  \"'s\",\n",
       "  'always',\n",
       "  'pepper',\n",
       "  'that',\n",
       "  'makes',\n",
       "  'people',\n",
       "  'hot-tempered',\n",
       "  '.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = '''I won't have any pepper in my kitchen at all. Soup does very well without-Maybe it's always pepper that makes people hot-tempered.'''\n",
    "[word_tokenize(t) for t in sent_tokenize(s3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-3: Use the following text for word tokenization: \n",
    "\n",
    "We had a meeting with our industrial partner on October 22nd, 2022, it was scheduled for 2 hours, but it ended up lasting for 2.5 hours. The meeting was productive and we were able to finalize the budget for next quarter, which is $250,000.\n",
    "\n",
    "- Implement a word tokenizer to tokenize the given text using a [regular expression](https://docs.python.org/3/library/re.html).\n",
    "- Perform word tokenization by using [NLTK word tokenizer](https://www.nltk.org/api/nltk.tokenize.html) and compare the results with the results generated with regular expression.\n",
    "\n",
    "Make sure to preserve all the numeric expressions and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['We',\n",
       "  'had',\n",
       "  'a',\n",
       "  'meeting',\n",
       "  'with',\n",
       "  'our',\n",
       "  'industrial',\n",
       "  'partner',\n",
       "  'on',\n",
       "  'October',\n",
       "  '22nd',\n",
       "  ',',\n",
       "  '2022',\n",
       "  ',',\n",
       "  'it',\n",
       "  'was',\n",
       "  'scheduled',\n",
       "  'for',\n",
       "  '2',\n",
       "  'hours',\n",
       "  ',',\n",
       "  'but',\n",
       "  'it',\n",
       "  'ended',\n",
       "  'up',\n",
       "  'lasting',\n",
       "  'for',\n",
       "  '2.5',\n",
       "  'hours',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'meeting',\n",
       "  'was',\n",
       "  'productive',\n",
       "  'and',\n",
       "  'we',\n",
       "  'were',\n",
       "  'able',\n",
       "  'to',\n",
       "  'finalize',\n",
       "  'the',\n",
       "  'budget',\n",
       "  'for',\n",
       "  'next',\n",
       "  'quarter',\n",
       "  ',',\n",
       "  'which',\n",
       "  'is',\n",
       "  '$',\n",
       "  '250,000',\n",
       "  '.']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = '''We had a meeting with our industrial partner on October 22nd, 2022, it was scheduled for 2 hours, but it ended up lasting for 2.5 hours. The meeting was productive and we were able to finalize the budget for next quarter, which is $250,000.'''\n",
    "[word_tokenize(t) for t in sent_tokenize(s4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-4: Use the following text for word tokenization: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a meeting with industrial partner and Ph.D researchers on October 22nd, 2022, it was scheduled for 2 hours, but it ended up lasting for 2.5 hours. The meeting was productive and we were able to finalize the budget for next quarter, which is $250,000. I won't have any pepper in my kitchen at all. Soup does very well without-Maybe it's always pepper that makes people hot-tempered.\n",
    "\n",
    "Implement a word tokenizer by using a [regular expression](https://docs.python.org/3/library/re.html) which has an ability to tokenize the given text with the following requirements:  \n",
    "\n",
    "- Keep intact the apostrophes, mutli-words, dates, and numerical expressions\n",
    "\n",
    "Compare the results of [NLTK's](https://www.nltk.org/api/nltk.tokenize.html) implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Task-1: Use the following text for noise / stop word removal: \n",
    "\n",
    "The life-span of trees is determined by growth rings. These can be seen if the tree is cut down or in cores\n",
    "taken from the edge to the center of the tree. Correct determination is only possible for trees which make\n",
    "growth rings, generally those which occur in seasonal climates. Trees in uniform non-seasonal tropical\n",
    "climates are always growing and do not have distinct growth rings. It is also only possible for trees which\n",
    "are solid to the center of the tree; many very old trees become hollow as the dead heartwood decays away.\n",
    "For some of these species, age estimates have been made on the basis of extrapolating current growth\n",
    "rates, but the results are usually little better than guesses or speculation. White proposed a method of\n",
    "estimating the age of large and veteran trees in the United Kingdom by correlation between a trees stem\n",
    "diameter, growth character and age.\n",
    "\n",
    "\n",
    "- Remove stop words from the gievn text by using [NLTK stop words list](https://www.nltk.org/search.html?q=stopwords&check_keywords=yes&area=default).\n",
    "- Add new stop words to default stop words list, and remove the stop words by using updated stop words list\n",
    "- Create a new file with name filtered_text, and save text has no stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life-span', 'trees', 'determined', 'growth', 'rings', '.', 'seen', 'tree', 'cut', 'cores', 'taken', 'edge', 'center', 'tree', '.', 'Correct', 'determination', 'possible', 'trees', 'make', 'growth', 'rings', ',', 'generally', 'occur', 'seasonal', 'climates', '.', 'Trees', 'uniform', 'non-seasonal', 'tropical', 'climates', 'always', 'growing', 'distinct', 'growth', 'rings', '.', 'also', 'possible', 'trees', 'solid', 'center', 'tree', ';', 'many', 'old', 'trees', 'become', 'hollow', 'dead', 'heartwood', 'decays', 'away', '.', 'species', ',', 'age', 'estimates', 'made', 'basis', 'extrapolating', 'current', 'growth', 'rates', ',', 'results', 'usually', 'little', 'better', 'guesses', 'speculation', '.', 'White', 'proposed', 'method', 'estimating', 'age', 'large', 'veteran', 'trees', 'United', 'Kingdom', 'correlation', 'trees', 'stem', 'diameter', ',', 'growth', 'character', 'age', '.']\n"
     ]
    }
   ],
   "source": [
    "s = '''The life-span of trees is determined by growth rings. These can be seen if the tree is cut down or in cores taken from the edge to the center of the tree. Correct determination is only possible for trees which make growth rings, generally those which occur in seasonal climates. Trees in uniform non-seasonal tropical climates are always growing and do not have distinct growth rings. It is also only possible for trees which are solid to the center of the tree; many very old trees become hollow as the dead heartwood decays away. For some of these species, age estimates have been made on the basis of extrapolating current growth rates, but the results are usually little better than guesses or speculation. White proposed a method of estimating the age of large and veteran trees in the United Kingdom by correlation between a trees stem diameter, growth character and age.'''\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "filtered_words = [word for word in word_tokenize(s) if word.lower() not in stopwords]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
